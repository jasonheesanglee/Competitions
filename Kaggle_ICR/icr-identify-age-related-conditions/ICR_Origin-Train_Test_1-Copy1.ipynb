{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b97f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7576cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ML\n",
    "from sklearn.ensemble import RandomForestClassifier  # Bagging\n",
    "from xgboost.sklearn import XGBClassifier            # GBM\n",
    "from sklearn.linear_model import LogisticRegression  # LogisticRegression\n",
    "from sklearn.svm import SVC                          # SVM\n",
    "from catboost import CatBoostClassifier as cat       # Catboost\n",
    "from lightgbm import LGBMClassifier as lgb           # LGBM\n",
    "\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, ReLU, Softmax, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# for checking multi-collinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# KFold(CV), partial : for optuna\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from functools import partial\n",
    "\n",
    "# AutoML framework\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c92b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configs\n",
    "is_tuning = True\n",
    "is_scaling = True\n",
    "is_pca = False\n",
    "if is_tuning:\n",
    "    n_trials=50\n",
    "    \n",
    "# Keras model compile\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e8decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc0ca812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_logloss(y_true, y_pred):\n",
    "#     y_pred = tf.clip_by_value(y_pred, 1e-15, 1-1e-15)\n",
    "#     y_pred /= tf.reduce_sum(y_pred, axis=1, keepdims=True)\n",
    "#     nc = tf.math.bincount(tf.cast(y_true, tf.int32))\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    y_pred /= np.sum(y_pred, axis=1)[:, None]\n",
    "    nc = np.bincount(y_true)\n",
    "  \n",
    "#     if tf.shape(nc)[0] == 1:\n",
    "#         return tf.constant(float('nan'))\n",
    "#     elif tf.shape(nc)[0] == 2:\n",
    "#         logloss = (-1 / nc[0] * (tf.reduc_sum(tf.where(y_true == 0, 1, 0) * tf.math.log(y_pred[:, 0])))\n",
    "#                    - 1 / nc[1] * (tf.reduce_sum(tf.where(y_true != 0, 1, 0) * tf.math.log(y_pred[:, 1])))) / 2\n",
    "#         return logloss\n",
    "        \n",
    "    \n",
    "    if len(nc) == 1:\n",
    "        return np.nan\n",
    "    elif len(nc) == 2:\n",
    "        logloss = (-1 / nc[0] * (np.sum(np.where(y_true == 0, 1, 0) * np.log(y_pred[:, 0])))\n",
    "                   - 1 / nc[1] * (np.sum(np.where(y_true != 0, 1, 0) * np.log(y_pred[:, 1])))) / 2\n",
    "        return logloss\n",
    "    else:\n",
    "        raise ValueError(\"Expected two classes in y_true.\")\n",
    "\n",
    "\n",
    "# def balance_loglossv2(y_true, y_pred):\n",
    "#     from sklearn.metrics import log_loss\n",
    "    \n",
    "#     target_mean = y_true.mean()\n",
    "#     w0 = 1/(1-target_mean)\n",
    "#     w1 = 1/target_mean\n",
    "#     sample_weight = [w0 if y == 0 else w1 for y in y_true]\n",
    "#     loss = log_loss(y_true, y_pred, sample_weight=sample_weight)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def b_logloss_keras(y_true, y_pred):\n",
    "    y_true = y_true[:, 1] * (1 - y_true[:, 0])\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "    score = tf.py_function(func=balance_logloss, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "greeks = pd.read_csv('./greeks.csv')\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c936de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>...</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>...</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>...</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>fd3dafe738fd</td>\n",
       "      <td>0.149555</td>\n",
       "      <td>3130.05946</td>\n",
       "      <td>123.763599</td>\n",
       "      <td>9.513984</td>\n",
       "      <td>13.020852</td>\n",
       "      <td>3.499305</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>8.545512</td>\n",
       "      <td>2.804172</td>\n",
       "      <td>...</td>\n",
       "      <td>9.879296</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.26092</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>8.967128</td>\n",
       "      <td>217.148554</td>\n",
       "      <td>8095.932828</td>\n",
       "      <td>24.640462</td>\n",
       "      <td>69.191944</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>fd895603f071</td>\n",
       "      <td>0.435846</td>\n",
       "      <td>5462.03438</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>46.551007</td>\n",
       "      <td>15.973224</td>\n",
       "      <td>5.979825</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>12.622906</td>\n",
       "      <td>3.777550</td>\n",
       "      <td>...</td>\n",
       "      <td>10.910227</td>\n",
       "      <td>10.223150</td>\n",
       "      <td>1.24236</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>35.896418</td>\n",
       "      <td>496.994214</td>\n",
       "      <td>3085.308063</td>\n",
       "      <td>29.648928</td>\n",
       "      <td>124.808872</td>\n",
       "      <td>0.145340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>fd8ef6377f76</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>2459.10720</td>\n",
       "      <td>130.138587</td>\n",
       "      <td>55.355778</td>\n",
       "      <td>10.005552</td>\n",
       "      <td>8.070549</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>15.408390</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>12.029366</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>19.962092</td>\n",
       "      <td>128.896894</td>\n",
       "      <td>6474.652866</td>\n",
       "      <td>26.166072</td>\n",
       "      <td>119.559420</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>fe1942975e40</td>\n",
       "      <td>0.363205</td>\n",
       "      <td>1263.53524</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>23.685856</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>7.981959</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>7.524588</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>8.026928</td>\n",
       "      <td>9.256996</td>\n",
       "      <td>0.78764</td>\n",
       "      <td>0.670527</td>\n",
       "      <td>24.594488</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>1965.343176</td>\n",
       "      <td>25.116750</td>\n",
       "      <td>37.155112</td>\n",
       "      <td>0.184622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>ffcca4ded3bb</td>\n",
       "      <td>0.482849</td>\n",
       "      <td>2672.53426</td>\n",
       "      <td>546.663930</td>\n",
       "      <td>112.006102</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.198099</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>7.948668</td>\n",
       "      <td>...</td>\n",
       "      <td>7.745765</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.14492</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>13.673940</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>6850.484442</td>\n",
       "      <td>45.745974</td>\n",
       "      <td>114.842372</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id        AB          AF          AH          AM         AR  \\\n",
       "0    000ff2bfdfe9  0.209377  3109.03329   85.200147   22.394407   8.138688   \n",
       "1    007255e47698  0.145282   978.76416   85.200147   36.968889   8.138688   \n",
       "2    013f2bd269f5  0.470030  2635.10654   85.200147   32.360553   8.138688   \n",
       "3    043ac50845d5  0.252107  3819.65177  120.201618   77.112203   8.138688   \n",
       "4    044fb8a146ec  0.380297  3733.04844   85.200147   14.103738   8.138688   \n",
       "..            ...       ...         ...         ...         ...        ...   \n",
       "612  fd3dafe738fd  0.149555  3130.05946  123.763599    9.513984  13.020852   \n",
       "613  fd895603f071  0.435846  5462.03438   85.200147   46.551007  15.973224   \n",
       "614  fd8ef6377f76  0.427300  2459.10720  130.138587   55.355778  10.005552   \n",
       "615  fe1942975e40  0.363205  1263.53524   85.200147   23.685856   8.138688   \n",
       "616  ffcca4ded3bb  0.482849  2672.53426  546.663930  112.006102   8.138688   \n",
       "\n",
       "           AX        AY         AZ          BC  ...         FI         FL  \\\n",
       "0    0.699861  0.025578   9.812214    5.555634  ...   3.583450   7.298162   \n",
       "1    3.632190  0.025578  13.517790    1.229900  ...  10.358927   0.173229   \n",
       "2    6.732840  0.025578  12.824570    1.229900  ...  11.626917   7.709560   \n",
       "3    3.685344  0.025578  11.053708    1.229900  ...  14.852022   6.122162   \n",
       "4    3.942255  0.054810   3.396778  102.151980  ...  13.666727   8.153058   \n",
       "..        ...       ...        ...         ...  ...        ...        ...   \n",
       "612  3.499305  0.077343   8.545512    2.804172  ...   9.879296   0.173229   \n",
       "613  5.979825  0.025882  12.622906    3.777550  ...  10.910227  10.223150   \n",
       "614  8.070549  0.025578  15.408390    1.229900  ...  12.029366   0.173229   \n",
       "615  7.981959  0.025578   7.524588    1.229900  ...   8.026928   9.256996   \n",
       "616  3.198099  0.116928   3.396778    7.948668  ...   7.745765   0.173229   \n",
       "\n",
       "           FR        FS         GB          GE            GF         GH  \\\n",
       "0     1.73855  0.094822  11.339138   72.611063   2003.810319  22.136229   \n",
       "1     0.49706  0.568932   9.292698   72.611063  27981.562750  29.135430   \n",
       "2     0.97556  1.198821  37.077772   88.609437  13676.957810  28.022851   \n",
       "3     0.49706  0.284466  18.529584   82.416803   2094.262452  39.948656   \n",
       "4    48.50134  0.121914  16.408728  146.109943   8524.370502  45.381316   \n",
       "..        ...       ...        ...         ...           ...        ...   \n",
       "612   1.26092  0.067730   8.967128  217.148554   8095.932828  24.640462   \n",
       "613   1.24236  0.426699  35.896418  496.994214   3085.308063  29.648928   \n",
       "614   0.49706  0.067730  19.962092  128.896894   6474.652866  26.166072   \n",
       "615   0.78764  0.670527  24.594488   72.611063   1965.343176  25.116750   \n",
       "616   1.14492  0.149006  13.673940   72.611063   6850.484442  45.745974   \n",
       "\n",
       "             GI         GL  \n",
       "0     69.834944   0.120343  \n",
       "1     32.131996  21.978000  \n",
       "2     35.192676   0.196941  \n",
       "3     90.493248   0.155829  \n",
       "4     36.262628   0.096614  \n",
       "..          ...        ...  \n",
       "612   69.191944  21.978000  \n",
       "613  124.808872   0.145340  \n",
       "614  119.559420  21.978000  \n",
       "615   37.155112   0.184622  \n",
       "616  114.842372  21.978000  \n",
       "\n",
       "[617 rows x 57 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_class = train.drop(columns='Class')\n",
    "train_no_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47157c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>611</th>\n",
       "      <th>612</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>007255e47698</td>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>04517a3c90bd</td>\n",
       "      <td>049232ca8356</td>\n",
       "      <td>057287f2da6d</td>\n",
       "      <td>0594b00fb30a</td>\n",
       "      <td>05f2bc0155cd</td>\n",
       "      <td>...</td>\n",
       "      <td>fb786fb02a65</td>\n",
       "      <td>fbb79ba9d642</td>\n",
       "      <td>fbc241daef00</td>\n",
       "      <td>fbd12c4ae88b</td>\n",
       "      <td>fd1dd68d51b4</td>\n",
       "      <td>fd3dafe738fd</td>\n",
       "      <td>fd895603f071</td>\n",
       "      <td>fd8ef6377f76</td>\n",
       "      <td>fe1942975e40</td>\n",
       "      <td>ffcca4ded3bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>G</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 617 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "Id     000ff2bfdfe9  007255e47698  013f2bd269f5  043ac50845d5  044fb8a146ec   \n",
       "Alpha             B             A             A             A             D   \n",
       "Beta              C             C             C             C             B   \n",
       "Gamma             G             M             M             M             F   \n",
       "Delta             D             B             B             B             B   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "Id     04517a3c90bd  049232ca8356  057287f2da6d  0594b00fb30a  05f2bc0155cd   \n",
       "Alpha             A             A             A             A             A   \n",
       "Beta              C             C             C             C             B   \n",
       "Gamma             M             M             M             M             M   \n",
       "Delta             B             B             B             B             B   \n",
       "\n",
       "       ...           607           608           609           610  \\\n",
       "Id     ...  fb786fb02a65  fbb79ba9d642  fbc241daef00  fbd12c4ae88b   \n",
       "Alpha  ...             A             B             A             A   \n",
       "Beta   ...             C             C             B             C   \n",
       "Gamma  ...             M             H             M             M   \n",
       "Delta  ...             B             B             B             C   \n",
       "\n",
       "                611           612           613           614           615  \\\n",
       "Id     fd1dd68d51b4  fd3dafe738fd  fd895603f071  fd8ef6377f76  fe1942975e40   \n",
       "Alpha             A             A             A             A             A   \n",
       "Beta              B             B             B             C             C   \n",
       "Gamma             M             M             M             M             M   \n",
       "Delta             B             B             B             B             B   \n",
       "\n",
       "                616  \n",
       "Id     ffcca4ded3bb  \n",
       "Alpha             A  \n",
       "Beta              C  \n",
       "Gamma             M  \n",
       "Delta             B  \n",
       "\n",
       "[5 rows x 617 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeks_cleanse = greeks.drop(columns='Epsilon').T\n",
    "greeks_cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c309b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FINAL_LIST = []\n",
    "for i in range(len(greeks_cleanse.T)):\n",
    "    temp = []\n",
    "\n",
    "    for value in greeks_cleanse[i]:\n",
    "        if len(value) > 1:\n",
    "            continue\n",
    "        else:\n",
    "            temp.append(value)\n",
    "    temp = \"\".join(temp)\n",
    "    TEMP_FINAL_LIST.append(temp)\n",
    "    \n",
    "temp_df = pd.DataFrame(TEMP_FINAL_LIST)\n",
    "\n",
    "temp_df.columns = ['Merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bcc7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "temp_df = label_encoder.fit_transform(temp_df['Merged'])\n",
    "meta_encoded = pd.DataFrame(temp_df)\n",
    "meta_encoded.columns = ['greeks_labeled']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55f3c938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>greeks_labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     greeks_labeled\n",
       "0                11\n",
       "1                 2\n",
       "2                 2\n",
       "3                 2\n",
       "4                19\n",
       "..              ...\n",
       "612               1\n",
       "613               1\n",
       "614               2\n",
       "615               2\n",
       "616               2\n",
       "\n",
       "[617 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2115cd",
   "metadata": {},
   "source": [
    "greeks_for_label_encoding = greeks.drop(columns=['Id', 'Epsilon'])\n",
    "for idx in greeks_for_label_encoding:\n",
    "    greeks_for_label_encoding[idx] = label_encoder.fit_transform(greeks_for_label_encoding[idx])\n",
    "\n",
    "train_test_2 = pd.concat([train_no_class, greeks_for_label_encoding], axis=1)\n",
    "train_test_2 = pd.concat([train_test_2, train['Class']], axis=1)\n",
    "train_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a5d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edfc7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "train.EJ = lb.fit_transform(train.EJ)  # A->0, B->1\n",
    "\n",
    "train_make = train.drop(columns=[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd34866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>...</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>...</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>...</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>...</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.149555</td>\n",
       "      <td>3130.05946</td>\n",
       "      <td>123.763599</td>\n",
       "      <td>9.513984</td>\n",
       "      <td>13.020852</td>\n",
       "      <td>3.499305</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>8.545512</td>\n",
       "      <td>2.804172</td>\n",
       "      <td>4157.68439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.26092</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>8.967128</td>\n",
       "      <td>217.148554</td>\n",
       "      <td>8095.932828</td>\n",
       "      <td>24.640462</td>\n",
       "      <td>69.191944</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.435846</td>\n",
       "      <td>5462.03438</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>46.551007</td>\n",
       "      <td>15.973224</td>\n",
       "      <td>5.979825</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>12.622906</td>\n",
       "      <td>3.777550</td>\n",
       "      <td>5654.07556</td>\n",
       "      <td>...</td>\n",
       "      <td>10.223150</td>\n",
       "      <td>1.24236</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>35.896418</td>\n",
       "      <td>496.994214</td>\n",
       "      <td>3085.308063</td>\n",
       "      <td>29.648928</td>\n",
       "      <td>124.808872</td>\n",
       "      <td>0.145340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.427300</td>\n",
       "      <td>2459.10720</td>\n",
       "      <td>130.138587</td>\n",
       "      <td>55.355778</td>\n",
       "      <td>10.005552</td>\n",
       "      <td>8.070549</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>15.408390</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5888.87769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>19.962092</td>\n",
       "      <td>128.896894</td>\n",
       "      <td>6474.652866</td>\n",
       "      <td>26.166072</td>\n",
       "      <td>119.559420</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0.363205</td>\n",
       "      <td>1263.53524</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>23.685856</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>7.981959</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>7.524588</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4517.86560</td>\n",
       "      <td>...</td>\n",
       "      <td>9.256996</td>\n",
       "      <td>0.78764</td>\n",
       "      <td>0.670527</td>\n",
       "      <td>24.594488</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>1965.343176</td>\n",
       "      <td>25.116750</td>\n",
       "      <td>37.155112</td>\n",
       "      <td>0.184622</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.482849</td>\n",
       "      <td>2672.53426</td>\n",
       "      <td>546.663930</td>\n",
       "      <td>112.006102</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.198099</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>7.948668</td>\n",
       "      <td>2818.01707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.14492</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>13.673940</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>6850.484442</td>\n",
       "      <td>45.745974</td>\n",
       "      <td>114.842372</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AB          AF          AH          AM         AR        AX  \\\n",
       "0    0.209377  3109.03329   85.200147   22.394407   8.138688  0.699861   \n",
       "1    0.145282   978.76416   85.200147   36.968889   8.138688  3.632190   \n",
       "2    0.470030  2635.10654   85.200147   32.360553   8.138688  6.732840   \n",
       "3    0.252107  3819.65177  120.201618   77.112203   8.138688  3.685344   \n",
       "4    0.380297  3733.04844   85.200147   14.103738   8.138688  3.942255   \n",
       "..        ...         ...         ...         ...        ...       ...   \n",
       "612  0.149555  3130.05946  123.763599    9.513984  13.020852  3.499305   \n",
       "613  0.435846  5462.03438   85.200147   46.551007  15.973224  5.979825   \n",
       "614  0.427300  2459.10720  130.138587   55.355778  10.005552  8.070549   \n",
       "615  0.363205  1263.53524   85.200147   23.685856   8.138688  7.981959   \n",
       "616  0.482849  2672.53426  546.663930  112.006102   8.138688  3.198099   \n",
       "\n",
       "           AY         AZ          BC         BD   ...         FL        FR  \\\n",
       "0    0.025578   9.812214    5.555634  4126.58731  ...   7.298162   1.73855   \n",
       "1    0.025578  13.517790    1.229900  5496.92824  ...   0.173229   0.49706   \n",
       "2    0.025578  12.824570    1.229900  5135.78024  ...   7.709560   0.97556   \n",
       "3    0.025578  11.053708    1.229900  4169.67738  ...   6.122162   0.49706   \n",
       "4    0.054810   3.396778  102.151980  5728.73412  ...   8.153058  48.50134   \n",
       "..        ...        ...         ...         ...  ...        ...       ...   \n",
       "612  0.077343   8.545512    2.804172  4157.68439  ...   0.173229   1.26092   \n",
       "613  0.025882  12.622906    3.777550  5654.07556  ...  10.223150   1.24236   \n",
       "614  0.025578  15.408390    1.229900  5888.87769  ...   0.173229   0.49706   \n",
       "615  0.025578   7.524588    1.229900  4517.86560  ...   9.256996   0.78764   \n",
       "616  0.116928   3.396778    7.948668  2818.01707  ...   0.173229   1.14492   \n",
       "\n",
       "           FS         GB          GE            GF         GH          GI  \\\n",
       "0    0.094822  11.339138   72.611063   2003.810319  22.136229   69.834944   \n",
       "1    0.568932   9.292698   72.611063  27981.562750  29.135430   32.131996   \n",
       "2    1.198821  37.077772   88.609437  13676.957810  28.022851   35.192676   \n",
       "3    0.284466  18.529584   82.416803   2094.262452  39.948656   90.493248   \n",
       "4    0.121914  16.408728  146.109943   8524.370502  45.381316   36.262628   \n",
       "..        ...        ...         ...           ...        ...         ...   \n",
       "612  0.067730   8.967128  217.148554   8095.932828  24.640462   69.191944   \n",
       "613  0.426699  35.896418  496.994214   3085.308063  29.648928  124.808872   \n",
       "614  0.067730  19.962092  128.896894   6474.652866  26.166072  119.559420   \n",
       "615  0.670527  24.594488   72.611063   1965.343176  25.116750   37.155112   \n",
       "616  0.149006  13.673940   72.611063   6850.484442  45.745974  114.842372   \n",
       "\n",
       "            GL  Class  \n",
       "0     0.120343    1.0  \n",
       "1    21.978000    0.0  \n",
       "2     0.196941    0.0  \n",
       "3     0.155829    0.0  \n",
       "4     0.096614    1.0  \n",
       "..         ...    ...  \n",
       "612  21.978000    0.0  \n",
       "613   0.145340    0.0  \n",
       "614  21.978000    0.0  \n",
       "615   0.184622    0.0  \n",
       "616  21.978000    0.0  \n",
       "\n",
       "[617 rows x 57 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imp = KNNImputer(n_neighbors=5)\n",
    "data = imp.fit_transform(train_make)\n",
    "train = pd.DataFrame(columns=train_make.columns,\n",
    "                    data=data)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c7b234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.2095739993551575,\n",
       " 5.470538251819652,\n",
       " 11.585741146057817,\n",
       " 2.786878254443682,\n",
       " 14.540495225807067,\n",
       " 10.228085450648146,\n",
       " 2.526156690473772,\n",
       " 9.966742100413436,\n",
       " 9.938931175170422,\n",
       " 23.227993233946783,\n",
       " 48.06760490475717,\n",
       " 4.1388138645341845,\n",
       " 3.4383694814886123,\n",
       " 1.2403515373222145,\n",
       " 8.946707585429914,\n",
       " 1.479249679578719,\n",
       " 13.683901431598526,\n",
       " 7.16315467623831,\n",
       " 3.1093321570638612,\n",
       " 20.53530663791221,\n",
       " 17.96706987015737,\n",
       " 11.037863968893861,\n",
       " 24.22647818878248,\n",
       " 13.220745704979937,\n",
       " 5.291024055996091,\n",
       " 11.731331611676834,\n",
       " 3.1850487770314944,\n",
       " 1.9149249477005985,\n",
       " 19.574883762985156,\n",
       " 8.295421294871216,\n",
       " 17.863211260857867,\n",
       " 19.66355500068502,\n",
       " 5.338229961461866,\n",
       " 50.46644497330504,\n",
       " 3.912479740208057,\n",
       " 13.312838778592633,\n",
       " 4.761066975281217,\n",
       " 2.1694803585135336,\n",
       " 29.6969369322989,\n",
       " 42.95100193888579,\n",
       " 5.877152122667336,\n",
       " 17.80985987647956,\n",
       " 1.5014021819363317,\n",
       " 1.7144306439608445,\n",
       " 23.74701805830803,\n",
       " 2.2656774598132055,\n",
       " 19.600839110286543,\n",
       " 3.075277608165649,\n",
       " 1.0510801038942414,\n",
       " 1.2131862685088506,\n",
       " 8.882930303012273,\n",
       " 3.0153376606737656,\n",
       " 2.505288212724215,\n",
       " 15.639370294339656,\n",
       " 4.55892634032048,\n",
       " 27.027286843282038,\n",
       " 2.337454129179655]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[variance_inflation_factor(train, i) for i in range (train.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71509e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vif(df):\n",
    "    vifs = [variance_inflation_factor(df, i) for i in range(df.shape[1])]\n",
    "    vif_df = pd.DataFrame({\"features\":df.columns, \"VIF\" : vifs})\n",
    "    vif_df = vif_df.sort_values(by=\"VIF\", ascending=False)\n",
    "    remove_col = vif_df.iloc[0, 0]\n",
    "    top_vif = vif_df.iloc[0, 1]\n",
    "    return vif_df, remove_col, top_vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4952664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DV 50.46644497330504\n",
      "BN 48.06110826582044\n",
      "EJ 32.6367510740489\n",
      "EH 29.40635981278969\n",
      "CS 24.144938821759094\n",
      "BD  20.702906700992884\n",
      "CH 19.316810821354846\n",
      "DH 18.654502700291943\n",
      "FI 17.585104766962257\n",
      "DN 17.042317022730487\n",
      "DL 16.010536364722082\n",
      "GH 13.074318172560375\n",
      "AR 12.852344213058096\n",
      "CC 12.038919689965544\n",
      "EB 11.810459819290948\n",
      "AX 9.892706105085827\n",
      "CU 9.028395892384713\n",
      "AZ 8.919414904629303\n",
      "DA 8.58855056939777\n",
      "CR 7.954673012680349\n",
      "BZ 7.6867848986619425\n",
      "EP 7.525379762629327\n",
      "GB 7.083289130542604\n",
      "DI 6.770878103410761\n",
      "AH 5.44579794497443\n",
      "AB 5.325827432481412\n",
      "EL 5.112489497216353\n",
      "CD  4.6443713088361624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF</th>\n",
       "      <th>AM</th>\n",
       "      <th>AY</th>\n",
       "      <th>BC</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>CB</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>...</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3109.03329</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>...</td>\n",
       "      <td>10.265073</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>978.76416</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2635.10654</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>...</td>\n",
       "      <td>8.745201</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3819.65177</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>...</td>\n",
       "      <td>7.884336</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3733.04844</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>...</td>\n",
       "      <td>4.274640</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>3130.05946</td>\n",
       "      <td>9.513984</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>2.804172</td>\n",
       "      <td>167.877117</td>\n",
       "      <td>27.287375</td>\n",
       "      <td>365.516874</td>\n",
       "      <td>41.368691</td>\n",
       "      <td>55.163024</td>\n",
       "      <td>4.780452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>17167.209610</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.26092</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>217.148554</td>\n",
       "      <td>8095.932828</td>\n",
       "      <td>69.191944</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>5462.03438</td>\n",
       "      <td>46.551007</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>3.777550</td>\n",
       "      <td>285.628059</td>\n",
       "      <td>344.644105</td>\n",
       "      <td>505.006814</td>\n",
       "      <td>61.910576</td>\n",
       "      <td>85.233928</td>\n",
       "      <td>6.682597</td>\n",
       "      <td>...</td>\n",
       "      <td>6.067614</td>\n",
       "      <td>18460.330020</td>\n",
       "      <td>10.223150</td>\n",
       "      <td>1.24236</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>496.994214</td>\n",
       "      <td>3085.308063</td>\n",
       "      <td>124.808872</td>\n",
       "      <td>0.145340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2459.10720</td>\n",
       "      <td>55.355778</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>178.661133</td>\n",
       "      <td>103.988995</td>\n",
       "      <td>2083.880500</td>\n",
       "      <td>90.411867</td>\n",
       "      <td>142.680216</td>\n",
       "      <td>7.809288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>5088.922912</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>128.896894</td>\n",
       "      <td>6474.652866</td>\n",
       "      <td>119.559420</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>1263.53524</td>\n",
       "      <td>23.685856</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>119.162529</td>\n",
       "      <td>82.512333</td>\n",
       "      <td>722.377629</td>\n",
       "      <td>12.499760</td>\n",
       "      <td>122.939496</td>\n",
       "      <td>2.964975</td>\n",
       "      <td>...</td>\n",
       "      <td>6.192291</td>\n",
       "      <td>6464.250832</td>\n",
       "      <td>9.256996</td>\n",
       "      <td>0.78764</td>\n",
       "      <td>0.670527</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>1965.343176</td>\n",
       "      <td>37.155112</td>\n",
       "      <td>0.184622</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2672.53426</td>\n",
       "      <td>112.006102</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>7.948668</td>\n",
       "      <td>306.127863</td>\n",
       "      <td>6.090490</td>\n",
       "      <td>747.474930</td>\n",
       "      <td>67.222974</td>\n",
       "      <td>271.240664</td>\n",
       "      <td>10.479286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>5895.352262</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.14492</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>6850.484442</td>\n",
       "      <td>114.842372</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AF          AM        AY          BC          BP          BQ  \\\n",
       "0    3109.03329   22.394407  0.025578    5.555634  175.638726  152.707705   \n",
       "1     978.76416   36.968889  0.025578    1.229900  155.868030   14.754720   \n",
       "2    2635.10654   32.360553  0.025578    1.229900  128.988531  219.320160   \n",
       "3    3819.65177   77.112203  0.025578    1.229900  237.282264   11.050410   \n",
       "4    3733.04844   14.103738  0.054810  102.151980  324.546318  149.717165   \n",
       "..          ...         ...       ...         ...         ...         ...   \n",
       "612  3130.05946    9.513984  0.077343    2.804172  167.877117   27.287375   \n",
       "613  5462.03438   46.551007  0.025882    3.777550  285.628059  344.644105   \n",
       "614  2459.10720   55.355778  0.025578    1.229900  178.661133  103.988995   \n",
       "615  1263.53524   23.685856  0.025578    1.229900  119.162529   82.512333   \n",
       "616  2672.53426  112.006102  0.116928    7.948668  306.127863    6.090490   \n",
       "\n",
       "              BR         CB         CD          CF  ...        FD   \\\n",
       "0     823.928241  47.223358   23.387600   4.851915  ...  10.265073   \n",
       "1      51.216883  30.284345   50.628208   6.085041  ...   0.296850   \n",
       "2     482.141594  32.563713   85.955376   5.376488  ...   8.745201   \n",
       "3     661.518640  15.201914   88.159360   2.347652  ...   7.884336   \n",
       "4    6074.859475  82.213495   72.644264  30.537722  ...   4.274640   \n",
       "..           ...        ...         ...        ...  ...        ...   \n",
       "612   365.516874  41.368691   55.163024   4.780452  ...   0.296850   \n",
       "613   505.006814  61.910576   85.233928   6.682597  ...   6.067614   \n",
       "614  2083.880500  90.411867  142.680216   7.809288  ...   0.296850   \n",
       "615   722.377629  12.499760  122.939496   2.964975  ...   6.192291   \n",
       "616   747.474930  67.222974  271.240664  10.479286  ...   0.296850   \n",
       "\n",
       "               FE         FL        FR        FS          GE            GF  \\\n",
       "0     9028.291921   7.298162   1.73855  0.094822   72.611063   2003.810319   \n",
       "1     6785.003474   0.173229   0.49706  0.568932   72.611063  27981.562750   \n",
       "2     8338.906181   7.709560   0.97556  1.198821   88.609437  13676.957810   \n",
       "3    10965.766040   6.122162   0.49706  0.284466   82.416803   2094.262452   \n",
       "4    16198.049590   8.153058  48.50134  0.121914  146.109943   8524.370502   \n",
       "..            ...        ...       ...       ...         ...           ...   \n",
       "612  17167.209610   0.173229   1.26092  0.067730  217.148554   8095.932828   \n",
       "613  18460.330020  10.223150   1.24236  0.426699  496.994214   3085.308063   \n",
       "614   5088.922912   0.173229   0.49706  0.067730  128.896894   6474.652866   \n",
       "615   6464.250832   9.256996   0.78764  0.670527   72.611063   1965.343176   \n",
       "616   5895.352262   0.173229   1.14492  0.149006   72.611063   6850.484442   \n",
       "\n",
       "             GI         GL  Class  \n",
       "0     69.834944   0.120343    1.0  \n",
       "1     32.131996  21.978000    0.0  \n",
       "2     35.192676   0.196941    0.0  \n",
       "3     90.493248   0.155829    0.0  \n",
       "4     36.262628   0.096614    1.0  \n",
       "..          ...        ...    ...  \n",
       "612   69.191944  21.978000    0.0  \n",
       "613  124.808872   0.145340    0.0  \n",
       "614  119.559420  21.978000    0.0  \n",
       "615   37.155112   0.184622    0.0  \n",
       "616  114.842372  21.978000    0.0  \n",
       "\n",
       "[617 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all features when VIF is over 10.\n",
    "top_vif = 100\n",
    "\n",
    "while(top_vif > 5):\n",
    "    vif_df, remove_col, top_vif = check_vif(train)\n",
    "    print(remove_col, top_vif)\n",
    "    if top_vif < 5:\n",
    "        break\n",
    "    train = train.drop(columns=remove_col)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6810b6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF</th>\n",
       "      <th>AM</th>\n",
       "      <th>AY</th>\n",
       "      <th>BC</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>CB</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>...</th>\n",
       "      <th>FE</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>greeks_labeled</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3109.03329</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>...</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>978.76416</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>...</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2635.10654</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>...</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3819.65177</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>...</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3733.04844</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>...</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>3130.05946</td>\n",
       "      <td>9.513984</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>2.804172</td>\n",
       "      <td>167.877117</td>\n",
       "      <td>27.287375</td>\n",
       "      <td>365.516874</td>\n",
       "      <td>41.368691</td>\n",
       "      <td>55.163024</td>\n",
       "      <td>4.780452</td>\n",
       "      <td>...</td>\n",
       "      <td>17167.209610</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.26092</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>217.148554</td>\n",
       "      <td>8095.932828</td>\n",
       "      <td>69.191944</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>5462.03438</td>\n",
       "      <td>46.551007</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>3.777550</td>\n",
       "      <td>285.628059</td>\n",
       "      <td>344.644105</td>\n",
       "      <td>505.006814</td>\n",
       "      <td>61.910576</td>\n",
       "      <td>85.233928</td>\n",
       "      <td>6.682597</td>\n",
       "      <td>...</td>\n",
       "      <td>18460.330020</td>\n",
       "      <td>10.223150</td>\n",
       "      <td>1.24236</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>496.994214</td>\n",
       "      <td>3085.308063</td>\n",
       "      <td>124.808872</td>\n",
       "      <td>0.145340</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2459.10720</td>\n",
       "      <td>55.355778</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>178.661133</td>\n",
       "      <td>103.988995</td>\n",
       "      <td>2083.880500</td>\n",
       "      <td>90.411867</td>\n",
       "      <td>142.680216</td>\n",
       "      <td>7.809288</td>\n",
       "      <td>...</td>\n",
       "      <td>5088.922912</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>128.896894</td>\n",
       "      <td>6474.652866</td>\n",
       "      <td>119.559420</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>1263.53524</td>\n",
       "      <td>23.685856</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>119.162529</td>\n",
       "      <td>82.512333</td>\n",
       "      <td>722.377629</td>\n",
       "      <td>12.499760</td>\n",
       "      <td>122.939496</td>\n",
       "      <td>2.964975</td>\n",
       "      <td>...</td>\n",
       "      <td>6464.250832</td>\n",
       "      <td>9.256996</td>\n",
       "      <td>0.78764</td>\n",
       "      <td>0.670527</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>1965.343176</td>\n",
       "      <td>37.155112</td>\n",
       "      <td>0.184622</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2672.53426</td>\n",
       "      <td>112.006102</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>7.948668</td>\n",
       "      <td>306.127863</td>\n",
       "      <td>6.090490</td>\n",
       "      <td>747.474930</td>\n",
       "      <td>67.222974</td>\n",
       "      <td>271.240664</td>\n",
       "      <td>10.479286</td>\n",
       "      <td>...</td>\n",
       "      <td>5895.352262</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.14492</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>6850.484442</td>\n",
       "      <td>114.842372</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AF          AM        AY          BC          BP          BQ  \\\n",
       "0    3109.03329   22.394407  0.025578    5.555634  175.638726  152.707705   \n",
       "1     978.76416   36.968889  0.025578    1.229900  155.868030   14.754720   \n",
       "2    2635.10654   32.360553  0.025578    1.229900  128.988531  219.320160   \n",
       "3    3819.65177   77.112203  0.025578    1.229900  237.282264   11.050410   \n",
       "4    3733.04844   14.103738  0.054810  102.151980  324.546318  149.717165   \n",
       "..          ...         ...       ...         ...         ...         ...   \n",
       "612  3130.05946    9.513984  0.077343    2.804172  167.877117   27.287375   \n",
       "613  5462.03438   46.551007  0.025882    3.777550  285.628059  344.644105   \n",
       "614  2459.10720   55.355778  0.025578    1.229900  178.661133  103.988995   \n",
       "615  1263.53524   23.685856  0.025578    1.229900  119.162529   82.512333   \n",
       "616  2672.53426  112.006102  0.116928    7.948668  306.127863    6.090490   \n",
       "\n",
       "              BR         CB         CD          CF  ...            FE  \\\n",
       "0     823.928241  47.223358   23.387600   4.851915  ...   9028.291921   \n",
       "1      51.216883  30.284345   50.628208   6.085041  ...   6785.003474   \n",
       "2     482.141594  32.563713   85.955376   5.376488  ...   8338.906181   \n",
       "3     661.518640  15.201914   88.159360   2.347652  ...  10965.766040   \n",
       "4    6074.859475  82.213495   72.644264  30.537722  ...  16198.049590   \n",
       "..           ...        ...         ...        ...  ...           ...   \n",
       "612   365.516874  41.368691   55.163024   4.780452  ...  17167.209610   \n",
       "613   505.006814  61.910576   85.233928   6.682597  ...  18460.330020   \n",
       "614  2083.880500  90.411867  142.680216   7.809288  ...   5088.922912   \n",
       "615   722.377629  12.499760  122.939496   2.964975  ...   6464.250832   \n",
       "616   747.474930  67.222974  271.240664  10.479286  ...   5895.352262   \n",
       "\n",
       "            FL        FR        FS          GE            GF          GI  \\\n",
       "0     7.298162   1.73855  0.094822   72.611063   2003.810319   69.834944   \n",
       "1     0.173229   0.49706  0.568932   72.611063  27981.562750   32.131996   \n",
       "2     7.709560   0.97556  1.198821   88.609437  13676.957810   35.192676   \n",
       "3     6.122162   0.49706  0.284466   82.416803   2094.262452   90.493248   \n",
       "4     8.153058  48.50134  0.121914  146.109943   8524.370502   36.262628   \n",
       "..         ...       ...       ...         ...           ...         ...   \n",
       "612   0.173229   1.26092  0.067730  217.148554   8095.932828   69.191944   \n",
       "613  10.223150   1.24236  0.426699  496.994214   3085.308063  124.808872   \n",
       "614   0.173229   0.49706  0.067730  128.896894   6474.652866  119.559420   \n",
       "615   9.256996   0.78764  0.670527   72.611063   1965.343176   37.155112   \n",
       "616   0.173229   1.14492  0.149006   72.611063   6850.484442  114.842372   \n",
       "\n",
       "            GL  greeks_labeled  Class  \n",
       "0     0.120343              11    1.0  \n",
       "1    21.978000               2    0.0  \n",
       "2     0.196941               2    0.0  \n",
       "3     0.155829               2    0.0  \n",
       "4     0.096614              19    1.0  \n",
       "..         ...             ...    ...  \n",
       "612  21.978000               1    0.0  \n",
       "613   0.145340               1    0.0  \n",
       "614  21.978000               2    0.0  \n",
       "615   0.184622               2    0.0  \n",
       "616  21.978000               2    0.0  \n",
       "\n",
       "[617 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_1 = pd.concat([train.drop(columns='Class'), meta_encoded], axis=1)\n",
    "train = pd.concat([train_test_1, train['Class']], axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97cd9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature selection via Feature Importance\n",
    "X = train.drop(columns=['Class'])\n",
    "y = train['Class']\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "# [(col, fi) for col, fi in zip(X.columns, rf.feature_importances_)]\n",
    "fi_df = pd.DataFrame({'feature': X.columns, 'importance' : rf.feature_importances_})\n",
    "\n",
    "fi_df.sort_values(by='importance', ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a74ade2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['greeks_labeled', 'DU', 'GL', 'FL', 'DE', 'AF', 'BC', 'FD ', 'FR',\n",
       "       'FE'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featrue 고름\n",
    "\n",
    "selected_cols = fi_df.sort_values(by='importance', ascending=False)[:10]['feature'].values\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e15c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b35b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 31) (509, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(216, 31)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class imbalance handling\n",
    "\n",
    "## 1. undersampling\n",
    "c1 = train[train.Class==1]\n",
    "c0 = train[train.Class==0]\n",
    "\n",
    "print(c1.shape, c0.shape)\n",
    "c0 = c0.sample(n=c1.shape[0])\n",
    "train = pd.concat([c0, c1])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "268b4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns = ['Class'])\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2381f09",
   "metadata": {},
   "source": [
    "##### Oversampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0c8df",
   "metadata": {},
   "source": [
    "df = train[selected_cols]\n",
    "df['Class'] = train['Class']\n",
    "pd.pivot_table(index='Class', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71123e46",
   "metadata": {},
   "source": [
    "## 2. oversampling = SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = train[selected_cols]\n",
    "y = train['Class']\n",
    "\n",
    "smote = SMOTE(k_neighbors=5)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "print(X_resampled.shape, y_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfa1fe",
   "metadata": {},
   "source": [
    "## 3. hybrid approach\n",
    "\n",
    "## class0 : 509 -> 300\n",
    "## class1 : 108 -> 300\n",
    "\n",
    "# class imbalance handling\n",
    "## 1. undersampling\n",
    "\n",
    "if sampling_method == 'hybrid':\n",
    "    N = 300\n",
    "    c1 = train[train.Class == 1]\n",
    "    c0 = train[train.Class == 0]\n",
    "    print(c1.shape, c0.shape)\n",
    "    c0 = c0.sample(n=N)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd14ea5",
   "metadata": {},
   "source": [
    "df = X_resampled.copy()\n",
    "df['Class'] = y_resampled\n",
    "pd.pivot_table(index='Class', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedd12e",
   "metadata": {},
   "source": [
    "X = X_resampled\n",
    "y = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0009c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 30) (33, 30) (183,) (33,)\n"
     ]
    }
   ],
   "source": [
    "# to make OOF prediction\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419cf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88e2696",
   "metadata": {},
   "source": [
    "##### Feature Scailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8664e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF</th>\n",
       "      <th>AM</th>\n",
       "      <th>AY</th>\n",
       "      <th>BC</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>CB</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>...</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>greeks_labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.252596</td>\n",
       "      <td>-0.366028</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.152155</td>\n",
       "      <td>-0.621232</td>\n",
       "      <td>-1.107102</td>\n",
       "      <td>-0.108624</td>\n",
       "      <td>-0.339861</td>\n",
       "      <td>-0.678668</td>\n",
       "      <td>-0.333418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108240</td>\n",
       "      <td>-0.618327</td>\n",
       "      <td>-0.178877</td>\n",
       "      <td>-0.095048</td>\n",
       "      <td>-0.762021</td>\n",
       "      <td>-0.118682</td>\n",
       "      <td>-0.706198</td>\n",
       "      <td>-1.251161</td>\n",
       "      <td>-0.721583</td>\n",
       "      <td>-0.808371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.117621</td>\n",
       "      <td>-0.353668</td>\n",
       "      <td>-0.070145</td>\n",
       "      <td>-0.152155</td>\n",
       "      <td>1.470145</td>\n",
       "      <td>-0.557862</td>\n",
       "      <td>-0.110309</td>\n",
       "      <td>-0.306822</td>\n",
       "      <td>0.248467</td>\n",
       "      <td>-0.261646</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132366</td>\n",
       "      <td>-0.367834</td>\n",
       "      <td>-0.533679</td>\n",
       "      <td>-0.087827</td>\n",
       "      <td>-0.668317</td>\n",
       "      <td>1.761972</td>\n",
       "      <td>-0.157587</td>\n",
       "      <td>1.133209</td>\n",
       "      <td>1.459620</td>\n",
       "      <td>-0.932267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.397176</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.092919</td>\n",
       "      <td>-0.209365</td>\n",
       "      <td>-0.820277</td>\n",
       "      <td>-0.107484</td>\n",
       "      <td>4.716102</td>\n",
       "      <td>0.919498</td>\n",
       "      <td>-0.083984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132366</td>\n",
       "      <td>-0.109861</td>\n",
       "      <td>-0.533679</td>\n",
       "      <td>-0.088592</td>\n",
       "      <td>0.409275</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>1.441697</td>\n",
       "      <td>-1.126268</td>\n",
       "      <td>1.459620</td>\n",
       "      <td>-0.808371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.504276</td>\n",
       "      <td>-0.197202</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.122596</td>\n",
       "      <td>-0.373858</td>\n",
       "      <td>-0.968494</td>\n",
       "      <td>0.009293</td>\n",
       "      <td>-0.143237</td>\n",
       "      <td>0.782902</td>\n",
       "      <td>-0.092760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116935</td>\n",
       "      <td>-0.418057</td>\n",
       "      <td>-0.496279</td>\n",
       "      <td>-0.094868</td>\n",
       "      <td>-0.637083</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>-0.629788</td>\n",
       "      <td>-0.809423</td>\n",
       "      <td>-0.021642</td>\n",
       "      <td>-0.808371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.116977</td>\n",
       "      <td>-0.052274</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.134059</td>\n",
       "      <td>-0.167656</td>\n",
       "      <td>-0.963180</td>\n",
       "      <td>-0.097013</td>\n",
       "      <td>-0.198098</td>\n",
       "      <td>-0.608757</td>\n",
       "      <td>-0.339364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>-0.638123</td>\n",
       "      <td>-0.028440</td>\n",
       "      <td>-0.086747</td>\n",
       "      <td>2.267732</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>1.780440</td>\n",
       "      <td>-0.217927</td>\n",
       "      <td>-0.705577</td>\n",
       "      <td>-0.808371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-0.430262</td>\n",
       "      <td>-0.330647</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.105181</td>\n",
       "      <td>-0.099251</td>\n",
       "      <td>-0.601638</td>\n",
       "      <td>-0.076778</td>\n",
       "      <td>-0.088759</td>\n",
       "      <td>-0.190602</td>\n",
       "      <td>-0.038259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072302</td>\n",
       "      <td>-0.308643</td>\n",
       "      <td>0.127586</td>\n",
       "      <td>-0.079488</td>\n",
       "      <td>1.611806</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>0.295882</td>\n",
       "      <td>-0.429885</td>\n",
       "      <td>-0.725154</td>\n",
       "      <td>0.554486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-0.135461</td>\n",
       "      <td>-0.333879</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.144603</td>\n",
       "      <td>0.257750</td>\n",
       "      <td>-0.875822</td>\n",
       "      <td>-0.041366</td>\n",
       "      <td>-0.162292</td>\n",
       "      <td>-0.564181</td>\n",
       "      <td>-0.163755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092206</td>\n",
       "      <td>-0.053598</td>\n",
       "      <td>-0.333869</td>\n",
       "      <td>-0.096072</td>\n",
       "      <td>0.081312</td>\n",
       "      <td>1.319480</td>\n",
       "      <td>-0.353198</td>\n",
       "      <td>1.193366</td>\n",
       "      <td>-0.650212</td>\n",
       "      <td>-0.932267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.568318</td>\n",
       "      <td>-0.232129</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.445526</td>\n",
       "      <td>-0.192026</td>\n",
       "      <td>-0.072077</td>\n",
       "      <td>-0.339861</td>\n",
       "      <td>-0.254777</td>\n",
       "      <td>-0.106563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132366</td>\n",
       "      <td>3.579011</td>\n",
       "      <td>-0.533679</td>\n",
       "      <td>-0.096072</td>\n",
       "      <td>-0.762021</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>-0.058297</td>\n",
       "      <td>-0.745040</td>\n",
       "      <td>1.459620</td>\n",
       "      <td>1.793446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.889754</td>\n",
       "      <td>3.101485</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>-0.152155</td>\n",
       "      <td>-0.266279</td>\n",
       "      <td>-0.179771</td>\n",
       "      <td>-0.142152</td>\n",
       "      <td>-0.331947</td>\n",
       "      <td>-0.408941</td>\n",
       "      <td>-0.348707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131562</td>\n",
       "      <td>0.484409</td>\n",
       "      <td>1.105255</td>\n",
       "      <td>-0.085411</td>\n",
       "      <td>2.580077</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>-0.452571</td>\n",
       "      <td>3.391842</td>\n",
       "      <td>-0.732516</td>\n",
       "      <td>0.058901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-0.143759</td>\n",
       "      <td>-0.274853</td>\n",
       "      <td>-0.100592</td>\n",
       "      <td>0.264779</td>\n",
       "      <td>-0.517853</td>\n",
       "      <td>-0.777520</td>\n",
       "      <td>-0.081429</td>\n",
       "      <td>-0.297678</td>\n",
       "      <td>-0.682014</td>\n",
       "      <td>-0.095379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113568</td>\n",
       "      <td>-0.475444</td>\n",
       "      <td>-0.295329</td>\n",
       "      <td>13.451241</td>\n",
       "      <td>-0.762021</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>-0.719006</td>\n",
       "      <td>-0.814847</td>\n",
       "      <td>-0.723034</td>\n",
       "      <td>0.926174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AF        AM        AY        BC        BP        BQ        BR  \\\n",
       "0   -1.252596 -0.366028 -0.100592 -0.152155 -0.621232 -1.107102 -0.108624   \n",
       "1   -0.117621 -0.353668 -0.070145 -0.152155  1.470145 -0.557862 -0.110309   \n",
       "2   -0.397176 -0.142088 -0.100592 -0.092919 -0.209365 -0.820277 -0.107484   \n",
       "3   -0.504276 -0.197202 -0.100592 -0.122596 -0.373858 -0.968494  0.009293   \n",
       "4   -0.116977 -0.052274 -0.100592 -0.134059 -0.167656 -0.963180 -0.097013   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "178 -0.430262 -0.330647 -0.100592 -0.105181 -0.099251 -0.601638 -0.076778   \n",
       "179 -0.135461 -0.333879 -0.100592 -0.144603  0.257750 -0.875822 -0.041366   \n",
       "180  0.568318 -0.232129 -0.100592 -0.087706 -0.445526 -0.192026 -0.072077   \n",
       "181  0.889754  3.101485 -0.100592 -0.152155 -0.266279 -0.179771 -0.142152   \n",
       "182 -0.143759 -0.274853 -0.100592  0.264779 -0.517853 -0.777520 -0.081429   \n",
       "\n",
       "           CB       CD         CF  ...       FD         FE        FL  \\\n",
       "0   -0.339861 -0.678668 -0.333418  ... -0.108240 -0.618327 -0.178877   \n",
       "1   -0.306822  0.248467 -0.261646  ... -0.132366 -0.367834 -0.533679   \n",
       "2    4.716102  0.919498 -0.083984  ... -0.132366 -0.109861 -0.533679   \n",
       "3   -0.143237  0.782902 -0.092760  ... -0.116935 -0.418057 -0.496279   \n",
       "4   -0.198098 -0.608757 -0.339364  ...  0.012842 -0.638123 -0.028440   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "178 -0.088759 -0.190602 -0.038259  ... -0.072302 -0.308643  0.127586   \n",
       "179 -0.162292 -0.564181 -0.163755  ... -0.092206 -0.053598 -0.333869   \n",
       "180 -0.339861 -0.254777 -0.106563  ... -0.132366  3.579011 -0.533679   \n",
       "181 -0.331947 -0.408941 -0.348707  ... -0.131562  0.484409  1.105255   \n",
       "182 -0.297678 -0.682014 -0.095379  ... -0.113568 -0.475444 -0.295329   \n",
       "\n",
       "            FR        FS        GE        GF        GI        GL  \\\n",
       "0    -0.095048 -0.762021 -0.118682 -0.706198 -1.251161 -0.721583   \n",
       "1    -0.087827 -0.668317  1.761972 -0.157587  1.133209  1.459620   \n",
       "2    -0.088592  0.409275 -0.393364  1.441697 -1.126268  1.459620   \n",
       "3    -0.094868 -0.637083 -0.393364 -0.629788 -0.809423 -0.021642   \n",
       "4    -0.086747  2.267732 -0.393364  1.780440 -0.217927 -0.705577   \n",
       "..         ...       ...       ...       ...       ...       ...   \n",
       "178  -0.079488  1.611806 -0.393364  0.295882 -0.429885 -0.725154   \n",
       "179  -0.096072  0.081312  1.319480 -0.353198  1.193366 -0.650212   \n",
       "180  -0.096072 -0.762021 -0.393364 -0.058297 -0.745040  1.459620   \n",
       "181  -0.085411  2.580077 -0.393364 -0.452571  3.391842 -0.732516   \n",
       "182  13.451241 -0.762021 -0.393364 -0.719006 -0.814847 -0.723034   \n",
       "\n",
       "     greeks_labeled  \n",
       "0         -0.808371  \n",
       "1         -0.932267  \n",
       "2         -0.808371  \n",
       "3         -0.808371  \n",
       "4         -0.808371  \n",
       "..              ...  \n",
       "178        0.554486  \n",
       "179       -0.932267  \n",
       "180        1.793446  \n",
       "181        0.058901  \n",
       "182        0.926174  \n",
       "\n",
       "[183 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if is_scaling:\n",
    "    scaler = StandardScaler()\n",
    "    data_ = scaler.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(data=data_, columns=X_train.columns)\n",
    "    data_ = scaler.transform(X_val)\n",
    "    X_val = pd.DataFrame(data=data_, columns=X_val.columns)\n",
    "    display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cdf7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_pca:\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components=0.80, random_state=42)\n",
    "    data_ = pca.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(data=data_, columns=[f\"PC{i}\" for i in range(1, data_.shape[1]+1)])\n",
    "    data_ = pca.transform(X_val)\n",
    "    X_val = pd.DataFrame(data=data_, columns=[f\"PC{i}\" for i in range(1, data_.shape[1]+1)])\n",
    "\n",
    "    display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a172690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5c874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b916eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                930       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 30)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 5)                 0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 5)                 0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 12        \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1857 (7.25 KB)\n",
      "Trainable params: 1857 (7.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model ensemble of SVM, Logistic Regression, XGBoost, RandomForest, Simple NN.\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "lr = LogisticRegression(random_state=42, max_iter=300)\n",
    "xgb = XGBClassifier(max_depth=3, colsample_bytree=0.8, reg_lambda=1, objective='binary:logistic', random_state=42)\n",
    "rf = RandomForestClassifier(max_depth=3, max_features=0.8, criterion='log_loss', random_state=42)\n",
    "catb = cat(iterations=300, depth=3, od_type='Iter', od_wait=15, bootstrap_type='Bayesian', random_state=42)\n",
    "lgbm = lgb(boosting_type='gbdt', min_child_samples=20, min_child_weight=0.001, n_estimators=20, random_state=42)\n",
    "\n",
    "nn = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(30), ReLU(), Dropout(0.2),\n",
    "    Dense(20), ReLU(), Dropout(0.2),\n",
    "    Dense(10), ReLU(), Dropout(0.1),\n",
    "    Dense(5), ReLU(), Dropout(0.2),\n",
    "    Dense(5), ReLU(), Dropout(0.2),\n",
    "    Dense(2), Softmax()\n",
    "])\n",
    "nn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0d3d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.8)   # [0.8, 0.2] <--> [0.9, 0] // [0, 0.9]\n",
    "scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=10,\n",
    "                              min_lr=1e-6)\n",
    "earlystopper = EarlyStopping(monitor='val_loss',\n",
    "                             patience=20,\n",
    "                             min_delta=1e-2)\n",
    "\n",
    "\n",
    "nn.compile(optimizer=optimizer, loss=loss_fn, metrics=[b_logloss_keras])\n",
    "\n",
    "nn_y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "nn_y_val = tf.keras.utils.to_categorical(y_val, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b3e6eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting LogisticRegression...\n",
      "\n",
      "Fitting SVM...\n",
      "\n",
      "Fitting RandomForest...\n",
      "\n",
      "Fitting XGBoost...\n",
      "\n",
      "Fitting CatBoost...\n",
      "Learning rate set to 0.015048\n",
      "0:\tlearn: 0.6604049\ttotal: 682us\tremaining: 204ms\n",
      "1:\tlearn: 0.6325064\ttotal: 1.58ms\tremaining: 236ms\n",
      "2:\tlearn: 0.6123846\ttotal: 2.4ms\tremaining: 238ms\n",
      "3:\tlearn: 0.5869539\ttotal: 2.96ms\tremaining: 219ms\n",
      "4:\tlearn: 0.5612997\ttotal: 3.54ms\tremaining: 209ms\n",
      "5:\tlearn: 0.5386461\ttotal: 4.08ms\tremaining: 200ms\n",
      "6:\tlearn: 0.5132830\ttotal: 4.63ms\tremaining: 194ms\n",
      "7:\tlearn: 0.4880770\ttotal: 5.16ms\tremaining: 188ms\n",
      "8:\tlearn: 0.4692198\ttotal: 5.57ms\tremaining: 180ms\n",
      "9:\tlearn: 0.4545128\ttotal: 6.26ms\tremaining: 181ms\n",
      "10:\tlearn: 0.4342132\ttotal: 6.85ms\tremaining: 180ms\n",
      "11:\tlearn: 0.4179094\ttotal: 7.43ms\tremaining: 178ms\n",
      "12:\tlearn: 0.4000218\ttotal: 7.79ms\tremaining: 172ms\n",
      "13:\tlearn: 0.3809425\ttotal: 8.15ms\tremaining: 167ms\n",
      "14:\tlearn: 0.3628931\ttotal: 8.55ms\tremaining: 163ms\n",
      "15:\tlearn: 0.3483508\ttotal: 8.92ms\tremaining: 158ms\n",
      "16:\tlearn: 0.3342985\ttotal: 9.31ms\tremaining: 155ms\n",
      "17:\tlearn: 0.3203188\ttotal: 9.73ms\tremaining: 152ms\n",
      "18:\tlearn: 0.3048175\ttotal: 10.2ms\tremaining: 151ms\n",
      "19:\tlearn: 0.2903922\ttotal: 10.7ms\tremaining: 150ms\n",
      "20:\tlearn: 0.2783938\ttotal: 11.2ms\tremaining: 149ms\n",
      "21:\tlearn: 0.2679568\ttotal: 11.7ms\tremaining: 148ms\n",
      "22:\tlearn: 0.2575624\ttotal: 12.1ms\tremaining: 146ms\n",
      "23:\tlearn: 0.2458466\ttotal: 12.6ms\tremaining: 145ms\n",
      "24:\tlearn: 0.2357972\ttotal: 13.1ms\tremaining: 144ms\n",
      "25:\tlearn: 0.2269910\ttotal: 13.6ms\tremaining: 143ms\n",
      "26:\tlearn: 0.2192286\ttotal: 13.9ms\tremaining: 141ms\n",
      "27:\tlearn: 0.2104776\ttotal: 14.4ms\tremaining: 140ms\n",
      "28:\tlearn: 0.2014572\ttotal: 14.9ms\tremaining: 139ms\n",
      "29:\tlearn: 0.1921058\ttotal: 15.6ms\tremaining: 140ms\n",
      "30:\tlearn: 0.1836901\ttotal: 16.5ms\tremaining: 143ms\n",
      "31:\tlearn: 0.1788102\ttotal: 17.4ms\tremaining: 146ms\n",
      "32:\tlearn: 0.1720919\ttotal: 18.8ms\tremaining: 152ms\n",
      "33:\tlearn: 0.1651886\ttotal: 19.3ms\tremaining: 151ms\n",
      "34:\tlearn: 0.1592427\ttotal: 19.8ms\tremaining: 150ms\n",
      "35:\tlearn: 0.1527006\ttotal: 20.6ms\tremaining: 151ms\n",
      "36:\tlearn: 0.1489642\ttotal: 21.2ms\tremaining: 151ms\n",
      "37:\tlearn: 0.1428139\ttotal: 21.6ms\tremaining: 149ms\n",
      "38:\tlearn: 0.1388813\ttotal: 21.9ms\tremaining: 147ms\n",
      "39:\tlearn: 0.1345087\ttotal: 22.3ms\tremaining: 145ms\n",
      "40:\tlearn: 0.1306652\ttotal: 22.7ms\tremaining: 144ms\n",
      "41:\tlearn: 0.1263551\ttotal: 23.4ms\tremaining: 144ms\n",
      "42:\tlearn: 0.1215401\ttotal: 23.8ms\tremaining: 142ms\n",
      "43:\tlearn: 0.1175811\ttotal: 24.3ms\tremaining: 141ms\n",
      "44:\tlearn: 0.1131393\ttotal: 24.7ms\tremaining: 140ms\n",
      "45:\tlearn: 0.1084153\ttotal: 25.2ms\tremaining: 139ms\n",
      "46:\tlearn: 0.1049176\ttotal: 25.6ms\tremaining: 138ms\n",
      "47:\tlearn: 0.1014445\ttotal: 26.2ms\tremaining: 138ms\n",
      "48:\tlearn: 0.0983512\ttotal: 26.7ms\tremaining: 137ms\n",
      "49:\tlearn: 0.0951318\ttotal: 27.2ms\tremaining: 136ms\n",
      "50:\tlearn: 0.0917750\ttotal: 27.6ms\tremaining: 135ms\n",
      "51:\tlearn: 0.0893389\ttotal: 28.1ms\tremaining: 134ms\n",
      "52:\tlearn: 0.0870964\ttotal: 28.6ms\tremaining: 133ms\n",
      "53:\tlearn: 0.0841412\ttotal: 29ms\tremaining: 132ms\n",
      "54:\tlearn: 0.0824708\ttotal: 29.5ms\tremaining: 131ms\n",
      "55:\tlearn: 0.0797250\ttotal: 29.9ms\tremaining: 130ms\n",
      "56:\tlearn: 0.0778864\ttotal: 30.4ms\tremaining: 129ms\n",
      "57:\tlearn: 0.0754226\ttotal: 30.8ms\tremaining: 129ms\n",
      "58:\tlearn: 0.0737969\ttotal: 31.3ms\tremaining: 128ms\n",
      "59:\tlearn: 0.0714928\ttotal: 32.1ms\tremaining: 128ms\n",
      "60:\tlearn: 0.0698756\ttotal: 32.5ms\tremaining: 127ms\n",
      "61:\tlearn: 0.0680560\ttotal: 33.2ms\tremaining: 127ms\n",
      "62:\tlearn: 0.0666469\ttotal: 33.6ms\tremaining: 126ms\n",
      "63:\tlearn: 0.0647385\ttotal: 34.2ms\tremaining: 126ms\n",
      "64:\tlearn: 0.0627532\ttotal: 34.8ms\tremaining: 126ms\n",
      "65:\tlearn: 0.0606725\ttotal: 35.4ms\tremaining: 125ms\n",
      "66:\tlearn: 0.0591012\ttotal: 35.8ms\tremaining: 125ms\n",
      "67:\tlearn: 0.0579827\ttotal: 36.2ms\tremaining: 124ms\n",
      "68:\tlearn: 0.0566904\ttotal: 36.7ms\tremaining: 123ms\n",
      "69:\tlearn: 0.0549649\ttotal: 37.2ms\tremaining: 122ms\n",
      "70:\tlearn: 0.0533498\ttotal: 37.8ms\tremaining: 122ms\n",
      "71:\tlearn: 0.0517553\ttotal: 38.6ms\tremaining: 122ms\n",
      "72:\tlearn: 0.0505580\ttotal: 39.3ms\tremaining: 122ms\n",
      "73:\tlearn: 0.0490989\ttotal: 39.8ms\tremaining: 121ms\n",
      "74:\tlearn: 0.0479411\ttotal: 40.2ms\tremaining: 121ms\n",
      "75:\tlearn: 0.0465500\ttotal: 40.8ms\tremaining: 120ms\n",
      "76:\tlearn: 0.0453352\ttotal: 45ms\tremaining: 130ms\n",
      "77:\tlearn: 0.0438138\ttotal: 45.7ms\tremaining: 130ms\n",
      "78:\tlearn: 0.0424405\ttotal: 46.2ms\tremaining: 129ms\n",
      "79:\tlearn: 0.0412732\ttotal: 46.9ms\tremaining: 129ms\n",
      "80:\tlearn: 0.0403909\ttotal: 47.4ms\tremaining: 128ms\n",
      "81:\tlearn: 0.0398357\ttotal: 47.9ms\tremaining: 127ms\n",
      "82:\tlearn: 0.0390264\ttotal: 49.7ms\tremaining: 130ms\n",
      "83:\tlearn: 0.0381143\ttotal: 50.4ms\tremaining: 130ms\n",
      "84:\tlearn: 0.0371337\ttotal: 51.3ms\tremaining: 130ms\n",
      "85:\tlearn: 0.0363232\ttotal: 51.8ms\tremaining: 129ms\n",
      "86:\tlearn: 0.0354002\ttotal: 52.3ms\tremaining: 128ms\n",
      "87:\tlearn: 0.0348776\ttotal: 52.7ms\tremaining: 127ms\n",
      "88:\tlearn: 0.0342134\ttotal: 53.2ms\tremaining: 126ms\n",
      "89:\tlearn: 0.0335394\ttotal: 53.6ms\tremaining: 125ms\n",
      "90:\tlearn: 0.0328712\ttotal: 54ms\tremaining: 124ms\n",
      "91:\tlearn: 0.0321859\ttotal: 54.6ms\tremaining: 123ms\n",
      "92:\tlearn: 0.0314924\ttotal: 55.1ms\tremaining: 123ms\n",
      "93:\tlearn: 0.0310397\ttotal: 55.7ms\tremaining: 122ms\n",
      "94:\tlearn: 0.0305720\ttotal: 56.4ms\tremaining: 122ms\n",
      "95:\tlearn: 0.0298463\ttotal: 57.1ms\tremaining: 121ms\n",
      "96:\tlearn: 0.0294559\ttotal: 57.6ms\tremaining: 121ms\n",
      "97:\tlearn: 0.0288724\ttotal: 58.3ms\tremaining: 120ms\n",
      "98:\tlearn: 0.0281269\ttotal: 58.9ms\tremaining: 120ms\n",
      "99:\tlearn: 0.0273936\ttotal: 59.6ms\tremaining: 119ms\n",
      "100:\tlearn: 0.0267926\ttotal: 60.4ms\tremaining: 119ms\n",
      "101:\tlearn: 0.0263957\ttotal: 60.8ms\tremaining: 118ms\n",
      "102:\tlearn: 0.0256617\ttotal: 61.3ms\tremaining: 117ms\n",
      "103:\tlearn: 0.0253442\ttotal: 61.7ms\tremaining: 116ms\n",
      "104:\tlearn: 0.0249028\ttotal: 62.6ms\tremaining: 116ms\n",
      "105:\tlearn: 0.0244465\ttotal: 63.4ms\tremaining: 116ms\n",
      "106:\tlearn: 0.0239275\ttotal: 64ms\tremaining: 115ms\n",
      "107:\tlearn: 0.0235358\ttotal: 64.7ms\tremaining: 115ms\n",
      "108:\tlearn: 0.0231509\ttotal: 65.4ms\tremaining: 115ms\n",
      "109:\tlearn: 0.0227489\ttotal: 66ms\tremaining: 114ms\n",
      "110:\tlearn: 0.0224026\ttotal: 66.5ms\tremaining: 113ms\n",
      "111:\tlearn: 0.0219129\ttotal: 67.1ms\tremaining: 113ms\n",
      "112:\tlearn: 0.0214863\ttotal: 67.5ms\tremaining: 112ms\n",
      "113:\tlearn: 0.0211718\ttotal: 68ms\tremaining: 111ms\n",
      "114:\tlearn: 0.0208353\ttotal: 68.4ms\tremaining: 110ms\n",
      "115:\tlearn: 0.0205264\ttotal: 69.1ms\tremaining: 110ms\n",
      "116:\tlearn: 0.0202023\ttotal: 70.6ms\tremaining: 110ms\n",
      "117:\tlearn: 0.0199590\ttotal: 71.7ms\tremaining: 111ms\n",
      "118:\tlearn: 0.0197312\ttotal: 72.4ms\tremaining: 110ms\n",
      "119:\tlearn: 0.0193986\ttotal: 73ms\tremaining: 110ms\n",
      "120:\tlearn: 0.0191281\ttotal: 73.6ms\tremaining: 109ms\n",
      "121:\tlearn: 0.0187798\ttotal: 74.1ms\tremaining: 108ms\n",
      "122:\tlearn: 0.0186189\ttotal: 74.6ms\tremaining: 107ms\n",
      "123:\tlearn: 0.0182465\ttotal: 75.2ms\tremaining: 107ms\n",
      "124:\tlearn: 0.0179143\ttotal: 75.7ms\tremaining: 106ms\n",
      "125:\tlearn: 0.0176745\ttotal: 76.1ms\tremaining: 105ms\n",
      "126:\tlearn: 0.0173768\ttotal: 76.5ms\tremaining: 104ms\n",
      "127:\tlearn: 0.0170684\ttotal: 76.9ms\tremaining: 103ms\n",
      "128:\tlearn: 0.0168462\ttotal: 77.4ms\tremaining: 103ms\n",
      "129:\tlearn: 0.0166033\ttotal: 77.9ms\tremaining: 102ms\n",
      "130:\tlearn: 0.0163162\ttotal: 79.2ms\tremaining: 102ms\n",
      "131:\tlearn: 0.0161153\ttotal: 80ms\tremaining: 102ms\n",
      "132:\tlearn: 0.0159028\ttotal: 80.7ms\tremaining: 101ms\n",
      "133:\tlearn: 0.0156516\ttotal: 81.2ms\tremaining: 101ms\n",
      "134:\tlearn: 0.0154466\ttotal: 82ms\tremaining: 100ms\n",
      "135:\tlearn: 0.0151577\ttotal: 82.6ms\tremaining: 99.6ms\n",
      "136:\tlearn: 0.0148372\ttotal: 83ms\tremaining: 98.8ms\n",
      "137:\tlearn: 0.0145985\ttotal: 83.5ms\tremaining: 98ms\n",
      "138:\tlearn: 0.0143613\ttotal: 83.9ms\tremaining: 97.2ms\n",
      "139:\tlearn: 0.0140958\ttotal: 84.4ms\tremaining: 96.5ms\n",
      "140:\tlearn: 0.0138786\ttotal: 84.9ms\tremaining: 95.7ms\n",
      "141:\tlearn: 0.0136269\ttotal: 85.5ms\tremaining: 95.1ms\n",
      "142:\tlearn: 0.0134058\ttotal: 86ms\tremaining: 94.4ms\n",
      "143:\tlearn: 0.0131961\ttotal: 86.4ms\tremaining: 93.6ms\n",
      "144:\tlearn: 0.0130897\ttotal: 86.9ms\tremaining: 92.8ms\n",
      "145:\tlearn: 0.0129292\ttotal: 87.7ms\tremaining: 92.5ms\n",
      "146:\tlearn: 0.0128076\ttotal: 88.2ms\tremaining: 91.8ms\n",
      "147:\tlearn: 0.0125565\ttotal: 89.5ms\tremaining: 91.9ms\n",
      "148:\tlearn: 0.0123694\ttotal: 90ms\tremaining: 91.3ms\n",
      "149:\tlearn: 0.0121431\ttotal: 90.7ms\tremaining: 90.7ms\n",
      "150:\tlearn: 0.0120341\ttotal: 91.2ms\tremaining: 90ms\n",
      "151:\tlearn: 0.0119431\ttotal: 91.6ms\tremaining: 89.2ms\n",
      "152:\tlearn: 0.0117963\ttotal: 92.1ms\tremaining: 88.4ms\n",
      "153:\tlearn: 0.0116420\ttotal: 92.6ms\tremaining: 87.8ms\n",
      "154:\tlearn: 0.0114765\ttotal: 93.2ms\tremaining: 87.2ms\n",
      "155:\tlearn: 0.0113466\ttotal: 94ms\tremaining: 86.8ms\n",
      "156:\tlearn: 0.0112076\ttotal: 95ms\tremaining: 86.5ms\n",
      "157:\tlearn: 0.0110710\ttotal: 95.5ms\tremaining: 85.8ms\n",
      "158:\tlearn: 0.0109181\ttotal: 95.9ms\tremaining: 85ms\n",
      "159:\tlearn: 0.0107777\ttotal: 96.9ms\tremaining: 84.8ms\n",
      "160:\tlearn: 0.0106305\ttotal: 98.8ms\tremaining: 85.3ms\n",
      "161:\tlearn: 0.0104725\ttotal: 99.8ms\tremaining: 85ms\n",
      "162:\tlearn: 0.0103759\ttotal: 101ms\tremaining: 84.8ms\n",
      "163:\tlearn: 0.0101961\ttotal: 101ms\tremaining: 83.9ms\n",
      "164:\tlearn: 0.0100440\ttotal: 102ms\tremaining: 83.1ms\n",
      "165:\tlearn: 0.0099524\ttotal: 102ms\tremaining: 82.3ms\n",
      "166:\tlearn: 0.0098572\ttotal: 102ms\tremaining: 81.5ms\n",
      "167:\tlearn: 0.0097123\ttotal: 103ms\tremaining: 80.7ms\n",
      "168:\tlearn: 0.0095661\ttotal: 103ms\tremaining: 80ms\n",
      "169:\tlearn: 0.0094981\ttotal: 104ms\tremaining: 79.2ms\n",
      "170:\tlearn: 0.0093601\ttotal: 104ms\tremaining: 78.5ms\n",
      "171:\tlearn: 0.0092721\ttotal: 105ms\tremaining: 77.8ms\n",
      "172:\tlearn: 0.0091390\ttotal: 105ms\tremaining: 77.1ms\n",
      "173:\tlearn: 0.0090331\ttotal: 105ms\tremaining: 76.3ms\n",
      "174:\tlearn: 0.0089192\ttotal: 106ms\tremaining: 75.6ms\n",
      "175:\tlearn: 0.0088018\ttotal: 106ms\tremaining: 74.9ms\n",
      "176:\tlearn: 0.0087404\ttotal: 107ms\tremaining: 74.2ms\n",
      "177:\tlearn: 0.0086783\ttotal: 107ms\tremaining: 73.6ms\n",
      "178:\tlearn: 0.0086569\ttotal: 108ms\tremaining: 73ms\n",
      "179:\tlearn: 0.0085776\ttotal: 109ms\tremaining: 72.4ms\n",
      "180:\tlearn: 0.0084592\ttotal: 109ms\tremaining: 71.7ms\n",
      "181:\tlearn: 0.0083864\ttotal: 110ms\tremaining: 71.1ms\n",
      "182:\tlearn: 0.0083054\ttotal: 110ms\tremaining: 70.5ms\n",
      "183:\tlearn: 0.0082136\ttotal: 111ms\tremaining: 70.1ms\n",
      "184:\tlearn: 0.0081267\ttotal: 112ms\tremaining: 69.6ms\n",
      "185:\tlearn: 0.0080104\ttotal: 113ms\tremaining: 69.1ms\n",
      "186:\tlearn: 0.0079251\ttotal: 113ms\tremaining: 68.5ms\n",
      "187:\tlearn: 0.0078531\ttotal: 114ms\tremaining: 67.8ms\n",
      "188:\tlearn: 0.0077919\ttotal: 114ms\tremaining: 67.1ms\n",
      "189:\tlearn: 0.0077177\ttotal: 115ms\tremaining: 66.5ms\n",
      "190:\tlearn: 0.0076454\ttotal: 115ms\tremaining: 65.8ms\n",
      "191:\tlearn: 0.0075881\ttotal: 116ms\tremaining: 65ms\n",
      "192:\tlearn: 0.0075480\ttotal: 116ms\tremaining: 64.3ms\n",
      "193:\tlearn: 0.0074866\ttotal: 116ms\tremaining: 63.6ms\n",
      "194:\tlearn: 0.0074325\ttotal: 117ms\tremaining: 62.9ms\n",
      "195:\tlearn: 0.0073753\ttotal: 117ms\tremaining: 62.2ms\n",
      "196:\tlearn: 0.0072821\ttotal: 118ms\tremaining: 61.6ms\n",
      "197:\tlearn: 0.0071989\ttotal: 118ms\tremaining: 60.9ms\n",
      "198:\tlearn: 0.0071559\ttotal: 119ms\tremaining: 60.2ms\n",
      "199:\tlearn: 0.0070797\ttotal: 119ms\tremaining: 59.7ms\n",
      "200:\tlearn: 0.0069831\ttotal: 120ms\tremaining: 59ms\n",
      "201:\tlearn: 0.0068999\ttotal: 120ms\tremaining: 58.3ms\n",
      "202:\tlearn: 0.0068086\ttotal: 121ms\tremaining: 57.6ms\n",
      "203:\tlearn: 0.0067592\ttotal: 121ms\tremaining: 57ms\n",
      "204:\tlearn: 0.0067183\ttotal: 121ms\tremaining: 56.3ms\n",
      "205:\tlearn: 0.0066534\ttotal: 122ms\tremaining: 55.6ms\n",
      "206:\tlearn: 0.0065908\ttotal: 123ms\tremaining: 55ms\n",
      "207:\tlearn: 0.0065255\ttotal: 124ms\tremaining: 55ms\n",
      "208:\tlearn: 0.0064642\ttotal: 127ms\tremaining: 55.2ms\n",
      "209:\tlearn: 0.0064061\ttotal: 127ms\tremaining: 54.5ms\n",
      "210:\tlearn: 0.0063673\ttotal: 128ms\tremaining: 53.8ms\n",
      "211:\tlearn: 0.0063058\ttotal: 128ms\tremaining: 53.1ms\n",
      "212:\tlearn: 0.0062431\ttotal: 128ms\tremaining: 52.5ms\n",
      "213:\tlearn: 0.0061930\ttotal: 129ms\tremaining: 51.8ms\n",
      "214:\tlearn: 0.0061912\ttotal: 129ms\tremaining: 51.2ms\n",
      "215:\tlearn: 0.0061423\ttotal: 130ms\tremaining: 50.5ms\n",
      "216:\tlearn: 0.0060894\ttotal: 130ms\tremaining: 49.8ms\n",
      "217:\tlearn: 0.0060358\ttotal: 131ms\tremaining: 49.2ms\n",
      "218:\tlearn: 0.0059973\ttotal: 131ms\tremaining: 48.5ms\n",
      "219:\tlearn: 0.0059399\ttotal: 132ms\tremaining: 47.9ms\n",
      "220:\tlearn: 0.0058957\ttotal: 132ms\tremaining: 47.3ms\n",
      "221:\tlearn: 0.0058455\ttotal: 133ms\tremaining: 46.6ms\n",
      "222:\tlearn: 0.0057973\ttotal: 133ms\tremaining: 45.9ms\n",
      "223:\tlearn: 0.0057552\ttotal: 134ms\tremaining: 45.3ms\n",
      "224:\tlearn: 0.0056909\ttotal: 134ms\tremaining: 44.7ms\n",
      "225:\tlearn: 0.0056883\ttotal: 134ms\tremaining: 44ms\n",
      "226:\tlearn: 0.0056379\ttotal: 135ms\tremaining: 43.4ms\n",
      "227:\tlearn: 0.0056047\ttotal: 135ms\tremaining: 42.8ms\n",
      "228:\tlearn: 0.0055456\ttotal: 136ms\tremaining: 42.1ms\n",
      "229:\tlearn: 0.0055394\ttotal: 136ms\tremaining: 41.5ms\n",
      "230:\tlearn: 0.0054960\ttotal: 137ms\tremaining: 40.8ms\n",
      "231:\tlearn: 0.0054586\ttotal: 137ms\tremaining: 40.2ms\n",
      "232:\tlearn: 0.0054135\ttotal: 138ms\tremaining: 39.6ms\n",
      "233:\tlearn: 0.0053690\ttotal: 138ms\tremaining: 39ms\n",
      "234:\tlearn: 0.0053272\ttotal: 139ms\tremaining: 38.4ms\n",
      "235:\tlearn: 0.0052886\ttotal: 139ms\tremaining: 37.8ms\n",
      "236:\tlearn: 0.0052570\ttotal: 140ms\tremaining: 37.3ms\n",
      "237:\tlearn: 0.0052120\ttotal: 141ms\tremaining: 36.6ms\n",
      "238:\tlearn: 0.0051840\ttotal: 141ms\tremaining: 36ms\n",
      "239:\tlearn: 0.0051309\ttotal: 141ms\tremaining: 35.4ms\n",
      "240:\tlearn: 0.0050974\ttotal: 142ms\tremaining: 34.8ms\n",
      "241:\tlearn: 0.0050913\ttotal: 142ms\tremaining: 34.1ms\n",
      "242:\tlearn: 0.0050386\ttotal: 143ms\tremaining: 33.6ms\n",
      "243:\tlearn: 0.0050082\ttotal: 144ms\tremaining: 33ms\n",
      "244:\tlearn: 0.0049783\ttotal: 144ms\tremaining: 32.4ms\n",
      "245:\tlearn: 0.0049575\ttotal: 145ms\tremaining: 31.8ms\n",
      "246:\tlearn: 0.0049382\ttotal: 145ms\tremaining: 31.1ms\n",
      "247:\tlearn: 0.0049079\ttotal: 146ms\tremaining: 30.5ms\n",
      "248:\tlearn: 0.0048763\ttotal: 146ms\tremaining: 29.9ms\n",
      "249:\tlearn: 0.0048502\ttotal: 146ms\tremaining: 29.3ms\n",
      "250:\tlearn: 0.0048245\ttotal: 147ms\tremaining: 28.7ms\n",
      "251:\tlearn: 0.0047940\ttotal: 148ms\tremaining: 28.1ms\n",
      "252:\tlearn: 0.0047598\ttotal: 148ms\tremaining: 27.5ms\n",
      "253:\tlearn: 0.0047313\ttotal: 149ms\tremaining: 27ms\n",
      "254:\tlearn: 0.0047158\ttotal: 150ms\tremaining: 26.5ms\n",
      "255:\tlearn: 0.0046869\ttotal: 151ms\tremaining: 25.9ms\n",
      "256:\tlearn: 0.0046429\ttotal: 151ms\tremaining: 25.3ms\n",
      "257:\tlearn: 0.0046100\ttotal: 152ms\tremaining: 24.7ms\n",
      "258:\tlearn: 0.0045773\ttotal: 152ms\tremaining: 24.1ms\n",
      "259:\tlearn: 0.0045415\ttotal: 153ms\tremaining: 23.5ms\n",
      "260:\tlearn: 0.0045193\ttotal: 153ms\tremaining: 22.9ms\n",
      "261:\tlearn: 0.0044807\ttotal: 153ms\tremaining: 22.3ms\n",
      "262:\tlearn: 0.0044492\ttotal: 154ms\tremaining: 21.7ms\n",
      "263:\tlearn: 0.0044173\ttotal: 155ms\tremaining: 21.1ms\n",
      "264:\tlearn: 0.0043951\ttotal: 155ms\tremaining: 20.5ms\n",
      "265:\tlearn: 0.0043694\ttotal: 156ms\tremaining: 19.9ms\n",
      "266:\tlearn: 0.0043665\ttotal: 156ms\tremaining: 19.3ms\n",
      "267:\tlearn: 0.0043286\ttotal: 157ms\tremaining: 18.7ms\n",
      "268:\tlearn: 0.0043047\ttotal: 157ms\tremaining: 18.1ms\n",
      "269:\tlearn: 0.0042740\ttotal: 158ms\tremaining: 17.5ms\n",
      "270:\tlearn: 0.0042405\ttotal: 158ms\tremaining: 16.9ms\n",
      "271:\tlearn: 0.0042169\ttotal: 158ms\tremaining: 16.3ms\n",
      "272:\tlearn: 0.0041895\ttotal: 159ms\tremaining: 15.7ms\n",
      "273:\tlearn: 0.0041689\ttotal: 160ms\tremaining: 15.2ms\n",
      "274:\tlearn: 0.0041496\ttotal: 160ms\tremaining: 14.6ms\n",
      "275:\tlearn: 0.0041132\ttotal: 161ms\tremaining: 14ms\n",
      "276:\tlearn: 0.0040786\ttotal: 161ms\tremaining: 13.4ms\n",
      "277:\tlearn: 0.0040586\ttotal: 162ms\tremaining: 12.8ms\n",
      "278:\tlearn: 0.0040280\ttotal: 163ms\tremaining: 12.3ms\n",
      "279:\tlearn: 0.0039951\ttotal: 165ms\tremaining: 11.8ms\n",
      "280:\tlearn: 0.0039659\ttotal: 165ms\tremaining: 11.2ms\n",
      "281:\tlearn: 0.0039402\ttotal: 166ms\tremaining: 10.6ms\n",
      "282:\tlearn: 0.0039397\ttotal: 166ms\tremaining: 10ms\n",
      "283:\tlearn: 0.0039101\ttotal: 167ms\tremaining: 9.41ms\n",
      "284:\tlearn: 0.0038892\ttotal: 168ms\tremaining: 8.83ms\n",
      "285:\tlearn: 0.0038613\ttotal: 169ms\tremaining: 8.25ms\n",
      "286:\tlearn: 0.0038383\ttotal: 169ms\tremaining: 7.67ms\n",
      "287:\tlearn: 0.0038177\ttotal: 170ms\tremaining: 7.08ms\n",
      "288:\tlearn: 0.0037928\ttotal: 170ms\tremaining: 6.48ms\n",
      "289:\tlearn: 0.0037709\ttotal: 171ms\tremaining: 5.89ms\n",
      "290:\tlearn: 0.0037453\ttotal: 171ms\tremaining: 5.29ms\n",
      "291:\tlearn: 0.0037154\ttotal: 172ms\tremaining: 4.71ms\n",
      "292:\tlearn: 0.0036891\ttotal: 175ms\tremaining: 4.19ms\n",
      "293:\tlearn: 0.0036870\ttotal: 176ms\tremaining: 3.59ms\n",
      "294:\tlearn: 0.0036641\ttotal: 176ms\tremaining: 2.99ms\n",
      "295:\tlearn: 0.0036434\ttotal: 177ms\tremaining: 2.39ms\n",
      "296:\tlearn: 0.0036182\ttotal: 178ms\tremaining: 1.79ms\n",
      "297:\tlearn: 0.0036041\ttotal: 178ms\tremaining: 1.2ms\n",
      "298:\tlearn: 0.0035875\ttotal: 179ms\tremaining: 597us\n",
      "299:\tlearn: 0.0035569\ttotal: 179ms\tremaining: 0us\n",
      "\n",
      "Fitting Light GBM...\n",
      "\n",
      "Fitting MLP...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 69ms/step - loss: 0.7234 - b_logloss_keras: 0.7483 - val_loss: 0.6968 - val_b_logloss_keras: 0.7071 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7146 - b_logloss_keras: 0.7291 - val_loss: 0.6948 - val_b_logloss_keras: 0.6988 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7124 - b_logloss_keras: 0.7260 - val_loss: 0.6939 - val_b_logloss_keras: 0.6942 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7249 - b_logloss_keras: 0.7565 - val_loss: 0.6937 - val_b_logloss_keras: 0.6929 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7077 - b_logloss_keras: 0.7259 - val_loss: 0.6937 - val_b_logloss_keras: 0.6935 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7166 - b_logloss_keras: 0.7400 - val_loss: 0.6937 - val_b_logloss_keras: 0.6942 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6997 - b_logloss_keras: 0.6842 - val_loss: 0.6937 - val_b_logloss_keras: 0.6946 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7047 - b_logloss_keras: 0.7205 - val_loss: 0.6937 - val_b_logloss_keras: 0.6949 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.7085 - b_logloss_keras: 0.7262 - val_loss: 0.6936 - val_b_logloss_keras: 0.6947 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7012 - b_logloss_keras: 0.7112 - val_loss: 0.6935 - val_b_logloss_keras: 0.6942 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFitting LogisticRegression...\")\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"\\nFitting SVM...\")\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"\\nFitting RandomForest...\")\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"\\nFitting XGBoost...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "print(\"\\nFitting CatBoost...\")\n",
    "catb.fit(X_train, y_train)\n",
    "print(\"\\nFitting Light GBM...\")\n",
    "lgbm.fit(X_train, y_train)\n",
    "print(\"\\nFitting MLP...\")\n",
    "history = nn.fit(X_train, nn_y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_data=[X_val, nn_y_val],\n",
    "                callbacks=[scheduler, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9d4b100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## loss visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2,1)\n",
    "plt.plot(history.history['loss'],'b-', label = \"training\")\n",
    "plt.plot(history.history['val_loss'], 'r:', label = \"validation\")\n",
    "plt.title(\"model - loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"model - val_logloss\")\n",
    "\n",
    "plt.plot(history.history['b_logloss_keras'], 'b-', label = \"training\")\n",
    "plt.plot(history.history['val_b_logloss_keras'], 'r:', label = \"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78d2e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metric\n",
    "evaluation_metric = balance_logloss\n",
    "evaluation_metric_keras = b_logloss_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6e248ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prediction with LR ---\n",
      "Train Score : 0.0527\n",
      "Test Score : 0.1101\n",
      "--- Prediction with SVM ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 183 into shape (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m svm_pred_train \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m     13\u001b[0m svm_pred_val \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m---> 15\u001b[0m svm_train_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvm_pred_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m svm_val_score \u001b[38;5;241m=\u001b[39m evaluation_metric(y_val, svm_pred_val)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Score : \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m svm_train_score)\n",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m, in \u001b[0;36mbalance_logloss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalance_logloss\u001b[39m(y_true, y_pred):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     y_pred = tf.clip_by_value(y_pred, 1e-15, 1-1e-15)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     y_pred /= tf.reduce_sum(y_pred, axis=1, keepdims=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     nc = tf.math.bincount(tf.cast(y_true, tf.int32))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(y_pred, \u001b[38;5;241m1e-15\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e-15\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     y_pred \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      8\u001b[0m     nc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 183 into shape (2)"
     ]
    }
   ],
   "source": [
    "print(\"--- Prediction with LR ---\")\n",
    "lr_pred_train = lr.predict_proba(X_train)\n",
    "lr_pred_val = lr.predict_proba(X_val)\n",
    "\n",
    "lr_train_score = evaluation_metric(y_train, lr_pred_train)\n",
    "lr_val_score = evaluation_metric(y_val, lr_pred_val)\n",
    "\n",
    "print(\"Train Score : %.4f\" % lr_train_score)\n",
    "print(\"Test Score : %.4f\" % lr_val_score)\n",
    "\n",
    "print(\"--- Prediction with SVM ---\")\n",
    "svm_pred_train = svm.predict(X_train)\n",
    "svm_pred_val = svm.predict(X_val)\n",
    "\n",
    "svm_train_score = evaluation_metric(y_train, svm_pred_train)\n",
    "svm_val_score = evaluation_metric(y_val, svm_pred_val)\n",
    "\n",
    "print(\"Train Score : %.4f\" % svm_train_score)\n",
    "print(\"Test Score : %.4f\" % svm_val_score)\n",
    "\n",
    "print(\"--- Prediction with RF ---\")\n",
    "rf_pred_train = rf.predict_proba(X_train)\n",
    "rf_pred_val = rf.predict_proba(X_val)\n",
    "\n",
    "rf_train_score = evaluation_metric(y_train, rf_pred_train)\n",
    "rf_val_score = evaluation_metric(y_val, rf_pred_val)\n",
    "\n",
    "print(\"Train Score : %.4f\" % rf_train_score)\n",
    "print(\"Test Score : %.4f\" % rf_val_score)\n",
    "\n",
    "print(\"--- Prediction with CAT ---\")\n",
    "cat_pred_train = catb.predict_proba(X_train)\n",
    "cat_pred_val = catb.predict_proba(X_val)\n",
    "\n",
    "cat_train_score = evaluation_metric(y_train, cat_pred_train)\n",
    "cat_val_score = evaluation_metric(y_val, cat_pred_val)\n",
    "\n",
    "print(\"Train Score : %.4f\" % cat_train_score)\n",
    "print(\"Test Score : %.4f\" % cat_val_score)\n",
    "\n",
    "print(\"--- Prediction with LGBM ---\")\n",
    "lgb_pred_train = lgbm.predict_proba(X_train)\n",
    "lgb_pred_val = lgbm.predict_proba(X_val)\n",
    "\n",
    "lgb_train_score = evaluation_metric(y_train, lgb_pred_train)\n",
    "lgb_val_score = evaluation_metric(y_val, lgb_pred_val)\n",
    "\n",
    "print(\"Train Score : %.4f\" % lgb_train_score)\n",
    "print(\"Test Score : %.4f\" % lgb_val_score)\n",
    "\n",
    "print(\"--- Prediction with MLP ---\")\n",
    "pred_train = nn.predict(X_train)\n",
    "pred_val = nn.predict(X_val)\n",
    "\n",
    "train_score = evaluation_metric_keras(nn_y_train, pred_train)\n",
    "val_score = evaluation_metric_keras(nn_y_val, pred_val)\n",
    "\n",
    "print(\"Train Score : %.4f\" % train_score)\n",
    "print(\"Validation Score : %.4f\" % val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6295a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52591381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8504eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_optimizer(trial, X, y, K):\n",
    "    # define parameter to tune\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 20)\n",
    "    max_features = trial.suggest_float('max_features', 0.6, 0.8, log=True)\n",
    "    \n",
    "    \n",
    "    # set model\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   max_features=max_features,\n",
    "                                   criterion='log_loss',\n",
    "                                   class_weight='balanced'\n",
    "                                  )\n",
    "    \n",
    "    # K-Fold Cross validation\n",
    "    folds = StratifiedKFold(n_splits=K)\n",
    "    losses = []\n",
    "    \n",
    "    for train_idx, val_idx in folds.split(X, y):\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        X_val = X.iloc[val_idx, :]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        loss = evaluation_metric(y_val, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    \n",
    "    # return mean score of CV\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0407749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_optimizer(trial, X, y, K):\n",
    "    C = trial.suggest_int('C', 1, 100)\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    \n",
    "\n",
    "    model = SVC(C=C,\n",
    "                kernel=kernel,\n",
    "                class_weight='balanced', # if class imbalanced\n",
    "                gamma=gamma,\n",
    "                probability=True,\n",
    "                cache_size=1000,\n",
    "                random_state=42\n",
    "               )\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=K)\n",
    "    losses = []\n",
    "    \n",
    "    for train_idx, val_idx in folds.split(X, y):\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        X_val = X.iloc[val_idx, :]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        loss = evaluation_metric(y_val, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bdf141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_optimizer(trial, X, y, K):\n",
    "\n",
    "    C = trial.suggest_int('C', 5, 100)\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'newton-cg', 'newton-cholesky', 'saga'])    \n",
    "    \n",
    "\n",
    "    model = LogisticRegression(C=C,\n",
    "                               solver=solver,\n",
    "                               max_iter=500,\n",
    "                               class_weight='balanced',\n",
    "                               random_state=42,\n",
    "                               n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=K)\n",
    "    losses = []\n",
    "    \n",
    "    for train_idx, val_idx in folds.split(X, y):\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        X_val = X.iloc[val_idx, :]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        loss = evaluation_metric(y_val, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d329b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_optimizer(trial, X, y, K):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 2000)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 20)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 0.8, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-2, log=True)\n",
    "    reg_lambda = trial.suggest_float('reg_lambda', 0.1, 2, log=True)\n",
    "    booster = trial.suggest_categorical('booster', ['gbtree', 'dart'])\n",
    "    \n",
    "    \n",
    "    model = XGBClassifier(n_estimators=n_estimators,\n",
    "                          max_depth=max_depth,\n",
    "                          booster=booster,\n",
    "                          colsample_bytree=colsample_bytree,\n",
    "                          learning_rate=learning_rate,\n",
    "                          reg_lambda=reg_lambda,\n",
    "                          scale_pos_weight=4.71)\n",
    "    \n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=K)\n",
    "    losses = []\n",
    "    \n",
    "    for train_idx, val_idx in folds.split(X, y):\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        X_val = X.iloc[val_idx, :]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        loss = evaluation_metric(y_val, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3ae7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_optimizer(trial, X, y, K):\n",
    "    iterations = trial.suggest_int('iterations', 50, 200)\n",
    "    depth = trial.suggest_int('depth', 4, 16)\n",
    "    bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-2, log=True)\n",
    "    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1e-8, 100.0, log=True)\n",
    "#     random_strength = trial.suggest_float('random_strength', 1e-8, 10.0, log=True)\n",
    "    bagging_temperature = trial.suggest_float('bagging_temperature', 0, 10, log=False)\n",
    "#     od_type = trial.suggest_categorical('od_type', ['IncToDec', 'Iter'])\n",
    "#     od_wait = trial.suggest_int('od_wait', 10, 50)\n",
    "    \n",
    "    \n",
    "    model = cat(iterations=iterations,\n",
    "                depth=depth,\n",
    "                bootstrap_type=bootstrap_type,\n",
    "                l2_leaf_reg=l2_leaf_reg,\n",
    "#                 random_strength=random_strength,\n",
    "                learning_rate=learning_rate,\n",
    "                bagging_temperature=bagging_temperature,\n",
    "#                 od_type=od_type,\n",
    "#                 od_wait=od_wait,\n",
    "#                 verbose=False,\n",
    "                random_state=42\n",
    "               )\n",
    "    \n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=K)\n",
    "    losses = []\n",
    "    \n",
    "    for train_idx, val_idx in folds.split(X, y):\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        X_val = X.iloc[val_idx, :]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        loss = evaluation_metric(y_val, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a3b976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_optimizer(trial, X, y, K):\n",
    "    num_leaves = trial.suggest_int('num_leaves', 100, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 20)\n",
    "    boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-2, log=True)\n",
    "    \n",
    "    \n",
    "    model = lgb(max_depth=max_depth,\n",
    "                application='binary',\n",
    "                metric='binary_logloss',\n",
    "                num_leaves=num_leaves,\n",
    "                boosting_type=boosting_type,\n",
    "                learning_rate=learning_rate,\n",
    "                random_state=42\n",
    "               )\n",
    "    \n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=K)\n",
    "    losses = []\n",
    "    \n",
    "    for train_idx, val_idx in folds.split(X, y):\n",
    "        X_train = X.iloc[train_idx, :]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        X_val = X.iloc[val_idx, :]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)\n",
    "        loss = evaluation_metric(y_val, preds)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6507963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 15:43:22,189] A new study created in memory with name: no-name-cc131346-43ca-4d2e-8b27-419abf40f105\n",
      "[I 2023-06-20 15:43:24,478] Trial 0 finished with value: 0.014144600549792491 and parameters: {'n_estimators': 445, 'max_depth': 16, 'max_features': 0.749541296308134}. Best is trial 0 with value: 0.014144600549792491.\n",
      "[I 2023-06-20 15:43:24,751] Trial 1 finished with value: 0.028995928177972485 and parameters: {'n_estimators': 52, 'max_depth': 4, 'max_features': 0.6219792350351198}. Best is trial 0 with value: 0.014144600549792491.\n",
      "[I 2023-06-20 15:43:26,068] Trial 2 finished with value: 0.015389705365469618 and parameters: {'n_estimators': 275, 'max_depth': 8, 'max_features': 0.7476196626016817}. Best is trial 0 with value: 0.014144600549792491.\n",
      "[I 2023-06-20 15:43:28,363] Trial 3 finished with value: 0.015025001617090266 and parameters: {'n_estimators': 481, 'max_depth': 19, 'max_features': 0.7570620975403644}. Best is trial 0 with value: 0.014144600549792491.\n",
      "[I 2023-06-20 15:43:29,404] Trial 4 finished with value: 0.013493305061468577 and parameters: {'n_estimators': 201, 'max_depth': 20, 'max_features': 0.7911781783702524}. Best is trial 4 with value: 0.013493305061468577.\n",
      "[I 2023-06-20 15:43:31,475] Trial 5 finished with value: 0.01704201586340297 and parameters: {'n_estimators': 406, 'max_depth': 15, 'max_features': 0.7167998479589678}. Best is trial 4 with value: 0.013493305061468577.\n",
      "[I 2023-06-20 15:43:31,884] Trial 6 finished with value: 0.020672223707659063 and parameters: {'n_estimators': 74, 'max_depth': 13, 'max_features': 0.7147715722023961}. Best is trial 4 with value: 0.013493305061468577.\n",
      "[I 2023-06-20 15:43:33,814] Trial 7 finished with value: 0.016516929401998417 and parameters: {'n_estimators': 388, 'max_depth': 12, 'max_features': 0.7165326911956347}. Best is trial 4 with value: 0.013493305061468577.\n",
      "[I 2023-06-20 15:43:35,781] Trial 8 finished with value: 0.012560007789144609 and parameters: {'n_estimators': 390, 'max_depth': 16, 'max_features': 0.7935706856722969}. Best is trial 8 with value: 0.012560007789144609.\n",
      "[I 2023-06-20 15:43:37,498] Trial 9 finished with value: 0.017013829191410895 and parameters: {'n_estimators': 353, 'max_depth': 20, 'max_features': 0.7065849011899292}. Best is trial 8 with value: 0.012560007789144609.\n",
      "[I 2023-06-20 15:43:39,227] Trial 10 finished with value: 0.014126112573746137 and parameters: {'n_estimators': 281, 'max_depth': 9, 'max_features': 0.7960658131355877}. Best is trial 8 with value: 0.012560007789144609.\n",
      "[I 2023-06-20 15:43:40,263] Trial 11 finished with value: 0.012201935240480017 and parameters: {'n_estimators': 176, 'max_depth': 17, 'max_features': 0.7867788520699974}. Best is trial 11 with value: 0.012201935240480017.\n",
      "[I 2023-06-20 15:43:41,323] Trial 12 finished with value: 0.012888279037850203 and parameters: {'n_estimators': 190, 'max_depth': 17, 'max_features': 0.7993192621442379}. Best is trial 11 with value: 0.012201935240480017.\n",
      "[I 2023-06-20 15:43:42,274] Trial 13 finished with value: 0.019517861509635876 and parameters: {'n_estimators': 159, 'max_depth': 13, 'max_features': 0.6747675563067675}. Best is trial 11 with value: 0.012201935240480017.\n",
      "[I 2023-06-20 15:43:44,066] Trial 14 finished with value: 0.012377155067979815 and parameters: {'n_estimators': 308, 'max_depth': 17, 'max_features': 0.7768234524303204}. Best is trial 11 with value: 0.012201935240480017.\n",
      "[I 2023-06-20 15:43:45,575] Trial 15 finished with value: 0.01200538900751263 and parameters: {'n_estimators': 275, 'max_depth': 18, 'max_features': 0.7676347109031765}. Best is trial 15 with value: 0.01200538900751263.\n",
      "[I 2023-06-20 15:43:46,344] Trial 16 finished with value: 0.014361955598018783 and parameters: {'n_estimators': 150, 'max_depth': 18, 'max_features': 0.7578406973986658}. Best is trial 15 with value: 0.01200538900751263.\n",
      "[I 2023-06-20 15:43:47,560] Trial 17 finished with value: 0.011986744128294218 and parameters: {'n_estimators': 242, 'max_depth': 14, 'max_features': 0.7679894323502603}. Best is trial 17 with value: 0.011986744128294218.\n",
      "[I 2023-06-20 15:43:49,169] Trial 18 finished with value: 0.012878161037509247 and parameters: {'n_estimators': 332, 'max_depth': 9, 'max_features': 0.7379485973997004}. Best is trial 17 with value: 0.011986744128294218.\n",
      "[I 2023-06-20 15:43:50,388] Trial 19 finished with value: 0.013511018603100921 and parameters: {'n_estimators': 251, 'max_depth': 14, 'max_features': 0.7688655379618501}. Best is trial 17 with value: 0.011986744128294218.\n",
      "[I 2023-06-20 15:43:51,643] Trial 20 finished with value: 0.01465610941520811 and parameters: {'n_estimators': 221, 'max_depth': 11, 'max_features': 0.7351712668477337}. Best is trial 17 with value: 0.011986744128294218.\n",
      "[I 2023-06-20 15:43:52,371] Trial 21 finished with value: 0.011465259638293243 and parameters: {'n_estimators': 112, 'max_depth': 18, 'max_features': 0.7746024130612138}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:52,946] Trial 22 finished with value: 0.012893920004650193 and parameters: {'n_estimators': 108, 'max_depth': 19, 'max_features': 0.7725779615900953}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:53,701] Trial 23 finished with value: 0.013556292632139436 and parameters: {'n_estimators': 130, 'max_depth': 15, 'max_features': 0.7676163493812638}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:55,082] Trial 24 finished with value: 0.012974357478084605 and parameters: {'n_estimators': 241, 'max_depth': 19, 'max_features': 0.7754283169905776}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:56,747] Trial 25 finished with value: 0.01380875404852347 and parameters: {'n_estimators': 299, 'max_depth': 18, 'max_features': 0.7340846606260013}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:57,279] Trial 26 finished with value: 0.016553282717336467 and parameters: {'n_estimators': 91, 'max_depth': 11, 'max_features': 0.7592766287067423}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:58,601] Trial 27 finished with value: 0.011994598937228622 and parameters: {'n_estimators': 240, 'max_depth': 15, 'max_features': 0.7765220344907999}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:43:59,855] Trial 28 finished with value: 0.011941577277196733 and parameters: {'n_estimators': 221, 'max_depth': 14, 'max_features': 0.7843796900334514}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:44:00,502] Trial 29 finished with value: 0.015221346697809222 and parameters: {'n_estimators': 126, 'max_depth': 6, 'max_features': 0.7497061839199779}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:44:01,584] Trial 30 finished with value: 0.012402398544592788 and parameters: {'n_estimators': 205, 'max_depth': 13, 'max_features': 0.789353257554447}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:44:02,911] Trial 31 finished with value: 0.012789630302265759 and parameters: {'n_estimators': 237, 'max_depth': 15, 'max_features': 0.7807741165707728}. Best is trial 21 with value: 0.011465259638293243.\n",
      "[I 2023-06-20 15:44:03,204] Trial 32 finished with value: 0.009980441080156789 and parameters: {'n_estimators': 50, 'max_depth': 14, 'max_features': 0.7812089080921466}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:03,549] Trial 33 finished with value: 0.010467883428581501 and parameters: {'n_estimators': 53, 'max_depth': 11, 'max_features': 0.7984761830071221}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:03,888] Trial 34 finished with value: 0.01306092830734931 and parameters: {'n_estimators': 51, 'max_depth': 11, 'max_features': 0.7996550582404739}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:04,299] Trial 35 finished with value: 0.013087062820973487 and parameters: {'n_estimators': 73, 'max_depth': 9, 'max_features': 0.7832533475731196}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:04,870] Trial 36 finished with value: 0.01583215313366954 and parameters: {'n_estimators': 100, 'max_depth': 10, 'max_features': 0.7542671346019644}. Best is trial 32 with value: 0.009980441080156789.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 15:44:05,282] Trial 37 finished with value: 0.013132123321559894 and parameters: {'n_estimators': 73, 'max_depth': 7, 'max_features': 0.7996085984480883}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:06,063] Trial 38 finished with value: 0.014047197265803313 and parameters: {'n_estimators': 135, 'max_depth': 12, 'max_features': 0.7860781649321685}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:08,799] Trial 39 finished with value: 0.016455850965380995 and parameters: {'n_estimators': 497, 'max_depth': 4, 'max_features': 0.7413384355022876}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:09,222] Trial 40 finished with value: 0.017906501671350442 and parameters: {'n_estimators': 72, 'max_depth': 16, 'max_features': 0.759450000022551}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:09,505] Trial 41 finished with value: 0.014336344698261254 and parameters: {'n_estimators': 52, 'max_depth': 14, 'max_features': 0.7847846267680789}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:10,039] Trial 42 finished with value: 0.014878818300989615 and parameters: {'n_estimators': 109, 'max_depth': 13, 'max_features': 0.7664768811148756}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:10,866] Trial 43 finished with value: 0.014572609216447825 and parameters: {'n_estimators': 169, 'max_depth': 14, 'max_features': 0.7485521627176699}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:11,293] Trial 44 finished with value: 0.011926720142495406 and parameters: {'n_estimators': 85, 'max_depth': 12, 'max_features': 0.78757019818318}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:11,763] Trial 45 finished with value: 0.014017968062341124 and parameters: {'n_estimators': 93, 'max_depth': 12, 'max_features': 0.7843913990982744}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:12,064] Trial 46 finished with value: 0.014988368013101426 and parameters: {'n_estimators': 55, 'max_depth': 10, 'max_features': 0.793291804233303}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:12,792] Trial 47 finished with value: 0.014382800958925033 and parameters: {'n_estimators': 144, 'max_depth': 16, 'max_features': 0.7997031239555963}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:14,915] Trial 48 finished with value: 0.012218932420356507 and parameters: {'n_estimators': 430, 'max_depth': 12, 'max_features': 0.7813674668511522}. Best is trial 32 with value: 0.009980441080156789.\n",
      "[I 2023-06-20 15:44:15,466] Trial 49 finished with value: 0.013393146382738518 and parameters: {'n_estimators': 112, 'max_depth': 10, 'max_features': 0.7901403504619016}. Best is trial 32 with value: 0.009980441080156789.\n"
     ]
    }
   ],
   "source": [
    "K = 6 # set K of K-Fold\n",
    "opt_func = partial(rf_optimizer, X=X_train, y=y_train, K=K)\n",
    "\n",
    "if is_tuning:\n",
    "    rf_study = optuna.create_study(direction=\"minimize\") # determine minimize or maximize sth\n",
    "    rf_study.optimize(opt_func, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36eb0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 15:44:15,471] A new study created in memory with name: no-name-12014680-241d-4968-8de5-ccd528ec381c\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:15,643] Trial 0 finished with value: 0.041169798704592676 and parameters: {'C': 48, 'solver': 'saga'}. Best is trial 0 with value: 0.041169798704592676.\n",
      "[I 2023-06-20 15:44:17,225] Trial 1 finished with value: 0.03888040195729634 and parameters: {'C': 75, 'solver': 'newton-cholesky'}. Best is trial 1 with value: 0.03888040195729634.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:17,254] Trial 2 finished with value: 0.032702412654406245 and parameters: {'C': 43, 'solver': 'liblinear'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:17,388] Trial 3 finished with value: 0.040768989084349366 and parameters: {'C': 69, 'solver': 'saga'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:17,517] Trial 4 finished with value: 0.04379155421578781 and parameters: {'C': 16, 'solver': 'saga'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:17,541] Trial 5 finished with value: 0.032941938078690815 and parameters: {'C': 38, 'solver': 'liblinear'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "[I 2023-06-20 15:44:17,925] Trial 6 finished with value: 0.0395673285451364 and parameters: {'C': 92, 'solver': 'newton-cg'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "[I 2023-06-20 15:44:18,313] Trial 7 finished with value: 0.03982141677981368 and parameters: {'C': 15, 'solver': 'newton-cg'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,335] Trial 8 finished with value: 0.0337578573157268 and parameters: {'C': 28, 'solver': 'liblinear'}. Best is trial 2 with value: 0.032702412654406245.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,361] Trial 9 finished with value: 0.03285986362892704 and parameters: {'C': 53, 'solver': 'liblinear'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "[I 2023-06-20 15:44:18,409] Trial 10 finished with value: 0.038555047991441556 and parameters: {'C': 65, 'solver': 'newton-cholesky'}. Best is trial 2 with value: 0.032702412654406245.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,440] Trial 11 finished with value: 0.03248643099295419 and parameters: {'C': 51, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,464] Trial 12 finished with value: 0.03304546788605455 and parameters: {'C': 36, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,488] Trial 13 finished with value: 0.03285986362892704 and parameters: {'C': 53, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,518] Trial 14 finished with value: 0.03289380906065901 and parameters: {'C': 84, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,549] Trial 15 finished with value: 0.033523905179846186 and parameters: {'C': 30, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:18,590] Trial 16 finished with value: 0.04565579973292264 and parameters: {'C': 6, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:18,989] Trial 17 finished with value: 0.038383477834393075 and parameters: {'C': 59, 'solver': 'newton-cholesky'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:19,040] Trial 18 finished with value: 0.038172098386142224 and parameters: {'C': 42, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,067] Trial 19 finished with value: 0.033137293143689794 and parameters: {'C': 100, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,097] Trial 20 finished with value: 0.03411222083374605 and parameters: {'C': 25, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,129] Trial 21 finished with value: 0.032512309644987115 and parameters: {'C': 50, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,167] Trial 22 finished with value: 0.03291951691196997 and parameters: {'C': 46, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,200] Trial 23 finished with value: 0.03281292800035713 and parameters: {'C': 60, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,232] Trial 24 finished with value: 0.032852926017346025 and parameters: {'C': 75, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,266] Trial 25 finished with value: 0.03282052665802948 and parameters: {'C': 56, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:19,417] Trial 26 finished with value: 0.04122703101864964 and parameters: {'C': 46, 'solver': 'saga'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:20,166] Trial 27 finished with value: 0.03811213509113831 and parameters: {'C': 34, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:20,206] Trial 28 finished with value: 0.038555047991441556 and parameters: {'C': 65, 'solver': 'newton-cholesky'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,330] Trial 29 finished with value: 0.041169798704592676 and parameters: {'C': 48, 'solver': 'saga'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,357] Trial 30 finished with value: 0.03274578426759601 and parameters: {'C': 42, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,388] Trial 31 finished with value: 0.03279026254376421 and parameters: {'C': 41, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,410] Trial 32 finished with value: 0.032512309644987115 and parameters: {'C': 50, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,446] Trial 33 finished with value: 0.032512309644987115 and parameters: {'C': 50, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,479] Trial 34 finished with value: 0.032512309644987115 and parameters: {'C': 50, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:20,534] Trial 35 finished with value: 0.03871311629524917 and parameters: {'C': 70, 'solver': 'newton-cholesky'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,561] Trial 36 finished with value: 0.03281292800035713 and parameters: {'C': 60, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,698] Trial 37 finished with value: 0.04081064528366811 and parameters: {'C': 66, 'solver': 'saga'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,729] Trial 38 finished with value: 0.03282409995631343 and parameters: {'C': 55, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:20,790] Trial 39 finished with value: 0.03914669592129236 and parameters: {'C': 78, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,819] Trial 40 finished with value: 0.034255722493495844 and parameters: {'C': 24, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,852] Trial 41 finished with value: 0.032512309644987115 and parameters: {'C': 50, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,882] Trial 42 finished with value: 0.032611896764896904 and parameters: {'C': 47, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,915] Trial 43 finished with value: 0.032941938078690815 and parameters: {'C': 38, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,954] Trial 44 finished with value: 0.03285886536953866 and parameters: {'C': 52, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:20,992] Trial 45 finished with value: 0.03281267973251927 and parameters: {'C': 58, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:21,119] Trial 46 finished with value: 0.041658476199677726 and parameters: {'C': 35, 'solver': 'saga'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:21,152] Trial 47 finished with value: 0.03281491992977588 and parameters: {'C': 62, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "[I 2023-06-20 15:44:21,196] Trial 48 finished with value: 0.03819454722778491 and parameters: {'C': 51, 'solver': 'newton-cholesky'}. Best is trial 11 with value: 0.03248643099295419.\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "[I 2023-06-20 15:44:21,228] Trial 49 finished with value: 0.03266008263707985 and parameters: {'C': 44, 'solver': 'liblinear'}. Best is trial 11 with value: 0.03248643099295419.\n"
     ]
    }
   ],
   "source": [
    "K = 6\n",
    "opt_func = partial(lr_optimizer, X=X_train, y=y_train, K=K)\n",
    "\n",
    "if is_tuning:\n",
    "    lr_study = optuna.create_study(direction=\"minimize\") \n",
    "    lr_study.optimize(opt_func, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "292949a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 15:44:21,234] A new study created in memory with name: no-name-37012144-200e-49c4-a275-4ad70020d9ee\n",
      "[I 2023-06-20 15:44:21,275] Trial 0 finished with value: 0.14829066156151746 and parameters: {'C': 84, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 0 with value: 0.14829066156151746.\n",
      "[I 2023-06-20 15:44:21,311] Trial 1 finished with value: 0.14705891970434035 and parameters: {'C': 56, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.14705891970434035.\n",
      "[I 2023-06-20 15:44:21,348] Trial 2 finished with value: 0.14414687162042253 and parameters: {'C': 6, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,384] Trial 3 finished with value: 0.14829066156151746 and parameters: {'C': 36, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,416] Trial 4 finished with value: 0.14829066156151746 and parameters: {'C': 48, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,457] Trial 5 finished with value: 0.14829066156151746 and parameters: {'C': 95, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,490] Trial 6 finished with value: 0.14837842908546217 and parameters: {'C': 13, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,526] Trial 7 finished with value: 0.1471881223791684 and parameters: {'C': 18, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,560] Trial 8 finished with value: 0.14840612656709296 and parameters: {'C': 20, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,599] Trial 9 finished with value: 0.14705891970434035 and parameters: {'C': 39, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,647] Trial 10 finished with value: 0.1591200900238118 and parameters: {'C': 3, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,691] Trial 11 finished with value: 0.14705891970434035 and parameters: {'C': 68, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,732] Trial 12 finished with value: 0.14705891970434035 and parameters: {'C': 68, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,775] Trial 13 finished with value: 0.14705891970434035 and parameters: {'C': 58, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,813] Trial 14 finished with value: 0.14829066156151746 and parameters: {'C': 32, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,851] Trial 15 finished with value: 0.14705891970434035 and parameters: {'C': 51, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,899] Trial 16 finished with value: 0.14829066156151746 and parameters: {'C': 74, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:21,953] Trial 17 finished with value: 0.14705891970434035 and parameters: {'C': 28, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,011] Trial 18 finished with value: 0.15671328217164318 and parameters: {'C': 3, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,053] Trial 19 finished with value: 0.14705891970434035 and parameters: {'C': 58, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,093] Trial 20 finished with value: 0.14705891970434035 and parameters: {'C': 42, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,137] Trial 21 finished with value: 0.14705891970434035 and parameters: {'C': 44, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,180] Trial 22 finished with value: 0.14705891970434035 and parameters: {'C': 26, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,220] Trial 23 finished with value: 0.14705891970434035 and parameters: {'C': 58, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,261] Trial 24 finished with value: 0.14725749384345674 and parameters: {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,301] Trial 25 finished with value: 0.14705891970434035 and parameters: {'C': 78, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,340] Trial 26 finished with value: 0.14829066156151746 and parameters: {'C': 37, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,379] Trial 27 finished with value: 0.14705891970434035 and parameters: {'C': 54, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,417] Trial 28 finished with value: 0.14829066156151746 and parameters: {'C': 65, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,461] Trial 29 finished with value: 0.14829066156151746 and parameters: {'C': 87, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,509] Trial 30 finished with value: 0.14705891970434035 and parameters: {'C': 42, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,556] Trial 31 finished with value: 0.14705891970434035 and parameters: {'C': 66, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,591] Trial 32 finished with value: 0.14705891970434035 and parameters: {'C': 85, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,627] Trial 33 finished with value: 0.14705891970434035 and parameters: {'C': 76, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,671] Trial 34 finished with value: 0.14705891970434035 and parameters: {'C': 62, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,712] Trial 35 finished with value: 0.14829066156151746 and parameters: {'C': 71, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,753] Trial 36 finished with value: 0.14829066156151746 and parameters: {'C': 100, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,799] Trial 37 finished with value: 0.14705891970434035 and parameters: {'C': 46, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,844] Trial 38 finished with value: 0.14829066156151746 and parameters: {'C': 81, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,889] Trial 39 finished with value: 0.14725749384345674 and parameters: {'C': 10, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,932] Trial 40 finished with value: 0.14715148297350292 and parameters: {'C': 23, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:22,977] Trial 41 finished with value: 0.14705891970434035 and parameters: {'C': 72, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,022] Trial 42 finished with value: 0.14705891970434035 and parameters: {'C': 68, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 15:44:23,069] Trial 43 finished with value: 0.14705891970434035 and parameters: {'C': 50, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,108] Trial 44 finished with value: 0.1468182816496706 and parameters: {'C': 16, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,148] Trial 45 finished with value: 0.14713165325604674 and parameters: {'C': 17, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,191] Trial 46 finished with value: 0.17532690002937465 and parameters: {'C': 1, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,230] Trial 47 finished with value: 0.14705891970434035 and parameters: {'C': 35, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,277] Trial 48 finished with value: 0.14414687162042253 and parameters: {'C': 6, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n",
      "[I 2023-06-20 15:44:23,318] Trial 49 finished with value: 0.14593298864277934 and parameters: {'C': 8, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 2 with value: 0.14414687162042253.\n"
     ]
    }
   ],
   "source": [
    "K = 6 \n",
    "opt_func = partial(svm_optimizer, X=X_train, y=y_train, K=K)\n",
    "\n",
    "if is_tuning:\n",
    "    svm_study = optuna.create_study(direction=\"minimize\") \n",
    "    svm_study.optimize(opt_func, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-20 15:44:23,326] A new study created in memory with name: no-name-fc164247-a6de-4e89-8ddc-0dfa38badb4c\n",
      "[I 2023-06-20 15:44:25,867] Trial 0 finished with value: 0.2948108155694273 and parameters: {'n_estimators': 673, 'max_depth': 16, 'colsample_bytree': 0.6933746053151664, 'learning_rate': 0.0012777560262082514, 'reg_lambda': 0.2696844583840411, 'booster': 'gbtree'}. Best is trial 0 with value: 0.2948108155694273.\n",
      "[I 2023-06-20 15:44:29,570] Trial 1 finished with value: 0.3229023032769975 and parameters: {'n_estimators': 851, 'max_depth': 7, 'colsample_bytree': 0.5087744895741833, 'learning_rate': 0.0011772835354095736, 'reg_lambda': 1.9144262291303373, 'booster': 'gbtree'}. Best is trial 0 with value: 0.2948108155694273.\n",
      "[I 2023-06-20 15:44:35,386] Trial 2 finished with value: 0.14924252101530633 and parameters: {'n_estimators': 1264, 'max_depth': 8, 'colsample_bytree': 0.5695507207052243, 'learning_rate': 0.0015237332357563706, 'reg_lambda': 0.2671941981726284, 'booster': 'gbtree'}. Best is trial 2 with value: 0.14924252101530633.\n"
     ]
    }
   ],
   "source": [
    "K = 6\n",
    "opt_func = partial(xgb_optimizer, X=X_train, y=y_train, K=K)\n",
    "\n",
    "if is_tuning:\n",
    "    xgb_study = optuna.create_study(direction=\"minimize\")\n",
    "    xgb_study.optimize(opt_func, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "opt_func = partial(cat_optimizer, X=X_train, y=y_train, K=K)\n",
    "\n",
    "if is_tuning:\n",
    "    cat_study = optuna.create_study(direction=\"minimize\")\n",
    "    cat_study.optimize(opt_func, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "opt_func = partial(lgbm_optimizer, X=X_train, y=y_train, K=K)\n",
    "\n",
    "if is_tuning:\n",
    "    lgbm_study = optuna.create_study(direction=\"minimize\")\n",
    "    lgbm_study.optimize(opt_func, n_trials=n_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90421f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all studies\n",
    "if is_tuning:\n",
    "    with open(\"rm_study.pk\", 'wb') as f:\n",
    "        pickle.dump(rf_study, f)\n",
    "    with open(\"lr_study.pk\", 'wb') as f:\n",
    "        pickle.dump(lr_study, f)\n",
    "    with open(\"svm_study.pk\", 'wb') as f:\n",
    "        pickle.dump(svm_study, f)\n",
    "    with open(\"xgb_study.pk\", 'wb') as f:\n",
    "        pickle.dump(xgb_study, f)\n",
    "    with open(\"cat_study.pk\", 'wb') as f:\n",
    "        pickle.dump(cat_study, f)\n",
    "    with open(\"lgbm_study.pk\", 'wb') as f:\n",
    "        pickle.dump(lgbm_study, f)\n",
    "        \n",
    "    nn.save(\"./simple_nn_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d26eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize experiment logs\n",
    "def display_experiment_log(study):\n",
    "    display(study.trials_dataframe())\n",
    "    print(\"Best Score: %.4f\" % study.best_value)\n",
    "    print(\"Best params: \", study.best_trial.params)\n",
    "    history = study.trials_dataframe()\n",
    "    display(history[history.value == study.best_value])\n",
    "    optuna.visualization.plot_optimization_history(study).show()\n",
    "    optuna.visualization.plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    display_experiment_log(rf_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    display_experiment_log(lr_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61534cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    display_experiment_log(svm_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b12e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    display_experiment_log(xgb_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    display_experiment_log(cat_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbe8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    display_experiment_log(lgbm_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94343f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing in same way\n",
    "X_test = test[train.columns.drop(\"Class\")].fillna(test.mean())\n",
    "if is_scaling:\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "if is_pca:\n",
    "    data_ = pca.transform(X_test)\n",
    "    X_test = pd.DataFrame(data=data_, columns=[f\"PC{i}\" for i in range(1, data_.shape[1]+1)])\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize Models\n",
    "if is_tuning:\n",
    "    rf_best_params = rf_study.best_params\n",
    "    lr_best_params = lr_study.best_params\n",
    "    xgb_best_params = xgb_study.best_params\n",
    "    svm_best_params = svm_study.best_params\n",
    "    lgbm_best_params = lgbm_study.best_params\n",
    "    cat_best_params = cat_study.best_params    \n",
    "    \n",
    "    best_rf = RandomForestClassifier(**rf_best_params)\n",
    "    best_lr = LogisticRegression(**lr_best_params)\n",
    "    best_xgb = XGBClassifier(**xgb_best_params)\n",
    "    best_svm = SVC(**svm_best_params, probability=True)\n",
    "    best_lgbm = lgb(**lgbm_best_params)\n",
    "    best_cat = cat(**cat_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n --------- RF ---------')\n",
    "print(rf_best_params)\n",
    "print(rf_study.best_value)\n",
    "\n",
    "print('\\n --------- LR ---------')\n",
    "print(lr_best_params)\n",
    "print(lr_study.best_value)\n",
    "\n",
    "print('\\n --------- XGB ---------')\n",
    "\n",
    "print(xgb_best_params)\n",
    "print(xgb_study.best_value)\n",
    "\n",
    "print('\\n --------- SVM ---------')\n",
    "print(svm_best_params)\n",
    "print(svm_study.best_value)\n",
    "\n",
    "print('\\n --------- LGB ---------')\n",
    "\n",
    "print(lgbm_best_params)\n",
    "print(lgbm_study.best_value)\n",
    "\n",
    "print('\\n --------- CAT ---------')\n",
    "\n",
    "print(cat_best_params)\n",
    "print(cat_study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda75025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8068a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c685ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95202cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bbc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first ensebmle model, then check it.\n",
    "best_rf.fit(X_train, y_train)\n",
    "best_lr.fit(X_train, y_train)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "best_svm.fit(X_train, y_train)\n",
    "best_lgbm.fit(X_train, y_train)\n",
    "best_cat.fit(X_train, y_train)\n",
    "\n",
    "# OOF-prediction\n",
    "v_rf = best_rf.predict_proba(X_val)\n",
    "v_lr = best_lr.predict_proba(X_val)\n",
    "v_xgb = best_xgb.predict_proba(X_val)\n",
    "v_svm = best_svm.predict_proba(X_val)\n",
    "v_lgbm = best_lgbm.predict_proba(X_val)\n",
    "v_cat = best_cat.predict_proba(X_val)\n",
    "\n",
    "print(v_rf.shape, v_lr.shape, v_xgb.shape, v_svm.shape, v_lgbm.shape, v_cat.shape)\n",
    "\n",
    "preds_rf = best_rf.predict_proba(X_test)\n",
    "preds_lr = best_lr.predict_proba(X_test)\n",
    "preds_xgb = best_xgb.predict_proba(X_test)\n",
    "preds_svm = best_svm.predict_proba(X_test)\n",
    "preds_lgbm = best_lgbm.predict_proba(X_test)\n",
    "preds_cat = best_cat.predict_proba(X_test)\n",
    "print(preds_rf.shape, preds_lr.shape, preds_xgb.shape, preds_svm.shape, preds_lgbm.shape, preds_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48528d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP predictions\n",
    "v_nn = nn.predict(X_val)\n",
    "preds_nn = nn.predict(X_test)\n",
    "print(v_nn.shape, preds_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF prediction\n",
    "ensembles = np.mean([v_xgb, v_svm, v_rf, v_lr, v_nn], axis=0)\n",
    "print(\"OOF prediction logloss : %.4f\" % evaluation_metric(y_val, ensembles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/icr-identify-age-related-conditions/sample_submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a318782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_weights = [0.1, 0.1, 0.25, 0.25, 0.3]\n",
    "# voting_weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "voting_weights = [0.10, 0.10, 0.10, 0.20, 0.20, 0.30]\n",
    "submission['class_0'] = voting_weights[0]*preds_nn[:, 0] + voting_weights[1]*preds_nn[:, 0] + voting_weights[2]*preds_nn[:, 0] + voting_weights[3]*preds_nn[:, 0] + voting_weights[4]*preds_nn[:, 0] + voting_weights[5]*preds_rf[:, 0] # + voting_weights[6]*preds_rf[:, 0]\n",
    "submission['class_1'] = voting_weights[0]*preds_nn[:, 1] + voting_weights[1]*preds_nn[:, 1] + voting_weights[2]*preds_nn[:, 1] + voting_weights[3]*preds_nn[:, 1] + voting_weights[4]*preds_nn[:, 1] + voting_weights[5]*preds_rf[:, 1] # + voting_weights[6]*preds_rf[:, 1]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ec356",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c710d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a391d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
